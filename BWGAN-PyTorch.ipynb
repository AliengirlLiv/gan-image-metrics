{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:42:42.857853Z",
     "start_time": "2019-01-28T06:42:42.438295Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:42:43.929533Z",
     "start_time": "2019-01-28T06:42:43.918183Z"
    },
    "code_folding": [
     0,
     3,
     6,
     20,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def upsample(x):\n",
    "    return nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "def downsample(x):\n",
    "    return nn.functional.interpolate(x, scale_factor=0.5, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "def conv_layer(in_filters, out_filters=32, kernel_size=3, he_init=True):\n",
    "    same_padding = (kernel_size-1)//2\n",
    "    conv = nn.Conv2d(in_filters, out_filters, kernel_size=kernel_size, padding=same_padding)\n",
    "    \n",
    "    if he_init:\n",
    "        he_init_constant = math.sqrt(6 / (in_filters * kernel_size**2))\n",
    "        nn.init.uniform_(conv.weight, -he_init_constant, he_init_constant)\n",
    "    else:\n",
    "        xavier_init_constant = math.sqrt(6 / ((in_filters + out_filters) * kernel_size**2))\n",
    "        nn.init.uniform_(conv.weight, -xavier_init_constant, xavier_init_constant)\n",
    "    nn.init.constant_(conv.bias, 0)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def bn(channels):\n",
    "    batchnorm = nn.BatchNorm2d(channels, eps=1e-5)\n",
    "    nn.init.constant_(batchnorm.weight, 1)\n",
    "    return batchnorm\n",
    "\n",
    "def linear(in_features, out_features):\n",
    "    linear_layer = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    xavier_init_constant = math.sqrt(6/(in_features+out_features))\n",
    "    nn.init.uniform_(linear_layer.weight, -xavier_init_constant, xavier_init_constant)\n",
    "    nn.init.constant_(linear_layer.bias, 0)\n",
    "    \n",
    "    return linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:42:45.120345Z",
     "start_time": "2019-01-28T06:42:45.105754Z"
    },
    "code_folding": [
     0,
     55
    ]
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, resample=None, normalize=False, activation=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.in_filters = in_filters\n",
    "        self.out_filters = out_filters\n",
    "        self.resample = resample\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.conv1 = conv_layer(in_filters, out_filters)\n",
    "        self.conv2 = conv_layer(out_filters, out_filters)\n",
    "        \n",
    "        if resample:\n",
    "            self.conv3 = conv_layer(in_filters, out_filters, kernel_size=1, he_init=False)\n",
    "        \n",
    "        if normalize:\n",
    "            self.bn1 = bn(in_filters)\n",
    "            self.bn2 = bn(out_filters)\n",
    "            \n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "                \n",
    "    def forward(self, x): \n",
    "        orig_input = x\n",
    "        \n",
    "        if self.normalize:\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if self.resample == 'up':\n",
    "            x = upsample(x)\n",
    "       \n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.bn2(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        if self.resample == 'down':\n",
    "            x = downsample(x)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.resample == 'down': \n",
    "            shortcut_x = downsample(self.conv3(orig_input))\n",
    "        elif self.resample == 'up':\n",
    "            shortcut_x = self.conv3(upsample(orig_input))\n",
    "        elif self.resample == None:\n",
    "            shortcut_x = orig_input\n",
    "        \n",
    "        return x + shortcut_x\n",
    "    \n",
    "class SmallResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, activation=None):\n",
    "        super(SmallResBlock, self).__init__()\n",
    "        self.in_filters = in_filters\n",
    "        self.out_filters = out_filters\n",
    "        \n",
    "        self.conv1 = conv_layer(in_filters, out_filters)\n",
    "        self.conv2 = conv_layer(out_filters, out_filters)\n",
    "        self.conv3 = conv_layer(in_filters, out_filters, kernel_size=1, he_init=False)\n",
    "            \n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "                \n",
    "    def forward(self, x): \n",
    "        orig_input = x\n",
    "       \n",
    "        x = self.conv1(x)   \n",
    "        x = downsample(self.conv2(self.activation(x)))\n",
    "\n",
    "        # Shortcut\n",
    "        shortcut_x = self.conv3(downsample(orig_input))\n",
    "        \n",
    "        return x + shortcut_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T07:10:48.328941Z",
     "start_time": "2019-01-28T07:10:48.308614Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, num_filters=128, num_blocks=3, start_image_size=4, num_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.num_filters = num_filters\n",
    "        self.start_image_size = start_image_size\n",
    "        \n",
    "        self.first_linear = linear(input_size, num_filters * start_image_size ** 2)\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        for _ in range(num_blocks):\n",
    "            self.resblocks.append(\n",
    "                ResBlock(in_filters=self.num_filters, \n",
    "                         out_filters=self.num_filters, \n",
    "                         resample='up', \n",
    "                         normalize=True))\n",
    "            \n",
    "        self.last_layer = conv_layer(num_filters, num_channels)\n",
    "        self.bn = bn(num_filters)\n",
    "        self.manually_initialize()\n",
    "    \n",
    "    def forward(self, noise):        \n",
    "        x = self.first_linear(noise)\n",
    "        \n",
    "        x = x.view(-1, self.start_image_size, self.start_image_size, self.num_filters)\n",
    "        x = x.permute(0, 3, 2, 1) # these last two lines could be simplified but tf consistency forces this.\n",
    "        \n",
    "        for resblock in self.resblocks:\n",
    "            x = resblock(x)\n",
    "            \n",
    "        x = self.activation(self.bn(x))\n",
    "        result = self.last_layer(x)\n",
    "        return torch.tanh(result)\n",
    "    \n",
    "    def manually_initialize(self):\n",
    "        self.first_linear.weight.data = torch.tensor(np.load(\"params/generator_dense_kernel:0.npy\")).transpose(0,1).contiguous()\n",
    "        self.first_linear.bias.data = torch.tensor(np.load(\"params/generator_dense_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].bn1.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_gamma:0.npy\"))\n",
    "        self.resblocks[0].bn1.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].bn2.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_1_gamma:0.npy\"))\n",
    "        self.resblocks[0].bn2.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_1_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].bn1.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_2_gamma:0.npy\"))\n",
    "        self.resblocks[1].bn1.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_2_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].bn2.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_3_gamma:0.npy\"))\n",
    "        self.resblocks[1].bn2.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_3_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].bn1.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_4_gamma:0.npy\"))\n",
    "        self.resblocks[2].bn1.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_4_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].bn2.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_5_gamma:0.npy\"))\n",
    "        self.resblocks[2].bn2.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_5_beta:0.npy\"))\n",
    "        \n",
    "        self.bn.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_6_gamma:0.npy\"))\n",
    "        self.bn.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_6_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv1.weight.data = torch.tensor(np.load(\"params/generator_conv2d_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv1.bias.data = torch.tensor(np.load(\"params/generator_conv2d_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv2.weight.data = torch.tensor(np.load(\"params/generator_conv2d_1_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv2.bias.data = torch.tensor(np.load(\"params/generator_conv2d_1_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv3.weight.data = torch.tensor(np.load(\"params/generator_conv2d_2_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv3.bias.data = torch.tensor(np.load(\"params/generator_conv2d_2_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv1.weight.data = torch.tensor(np.load(\"params/generator_conv2d_3_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv1.bias.data = torch.tensor(np.load(\"params/generator_conv2d_3_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv2.weight.data = torch.tensor(np.load(\"params/generator_conv2d_4_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv2.bias.data = torch.tensor(np.load(\"params/generator_conv2d_4_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv3.weight.data = torch.tensor(np.load(\"params/generator_conv2d_5_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv3.bias.data = torch.tensor(np.load(\"params/generator_conv2d_5_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv1.weight.data = torch.tensor(np.load(\"params/generator_conv2d_6_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv1.bias.data = torch.tensor(np.load(\"params/generator_conv2d_6_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv2.weight.data = torch.tensor(np.load(\"params/generator_conv2d_7_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv2.bias.data = torch.tensor(np.load(\"params/generator_conv2d_7_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv3.weight.data = torch.tensor(np.load(\"params/generator_conv2d_8_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv3.bias.data = torch.tensor(np.load(\"params/generator_conv2d_8_bias:0.npy\"))\n",
    "        \n",
    "        self.last_layer.weight.data = torch.tensor(np.load(\"params/generator_conv2d_9_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.last_layer.bias.data = torch.tensor(np.load(\"params/generator_conv2d_9_bias:0.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:42:47.893230Z",
     "start_time": "2019-01-28T06:42:47.884525Z"
    },
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.block = nn.Sequential(linear(input_size, output_size), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class VectorDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_blocks):\n",
    "        super(VectorDiscriminator, self).__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.blocks.append(MLPLayer(input_size, hidden_size))\n",
    "        \n",
    "        for _ in range(num_blocks - 1):\n",
    "            self.blocks.append(hidden_size, hidden_size)\n",
    "        \n",
    "        self.blocks.append(linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T07:10:52.158375Z",
     "start_time": "2019-01-28T07:10:52.141235Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ImageDiscriminator(nn.Module):\n",
    "    def __init__(self, num_filters=128, num_blocks=4, num_channels=3):\n",
    "        super(ImageDiscriminator, self).__init__()\n",
    "        assert num_blocks >= 2, \"Number of conv layers in the discriminator must be >= 2.\"\n",
    "        \n",
    "        self.resblocks = nn.ModuleList()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.resblocks.append(SmallResBlock(in_filters=num_channels, \n",
    "                                            out_filters=num_filters))\n",
    "        self.resblocks.append(ResBlock(in_filters=num_filters, \n",
    "                                       out_filters=num_filters, \n",
    "                                       resample='down'))\n",
    "        for _ in range(num_blocks - 2):\n",
    "            self.resblocks.append(ResBlock(in_filters=num_filters, \n",
    "                                           out_filters=num_filters))\n",
    "            \n",
    "        self.last_linear = linear(num_filters, 1)\n",
    "        self.manually_initialize()\n",
    "   \n",
    "    def forward(self, x):\n",
    "        for resblock in self.resblocks:\n",
    "            x = resblock(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        x = x.mean(dim=(-1,-2))\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "    \n",
    "    def manually_initialize(self):\n",
    "        self.last_linear.weight.data = torch.tensor(np.load(\"params/discriminator_dense_kernel:0.npy\")).transpose(0,1).contiguous()\n",
    "        self.last_linear.bias.data = torch.tensor(np.load(\"params/discriminator_dense_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_1_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_1_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv3.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_2_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv3.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_2_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_3_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_3_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_4_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_4_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv3.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_5_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv3.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_5_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_6_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_6_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_7_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_7_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[3].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_8_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[3].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_8_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[3].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_9_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[3].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_9_bias:0.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:48:30.331377Z",
     "start_time": "2019-01-28T06:48:30.314881Z"
    },
    "code_folding": [
     0,
     3,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def identity_embedding(pic):\n",
    "    return pic\n",
    "\n",
    "class WGAN(nn.Module):\n",
    "    def __init__(self, gamma, noise_size=128, num_filters=128, num_generator_blocks=3, num_discriminator_blocks=4,\n",
    "                 batch_size=64, num_channels=3, discriminator_epsilon=1e-5, embedding_size=None, \n",
    "                 discriminator_hidden_size=None, discriminator_type=\"Image\"):  \n",
    "        super(WGAN, self).__init__()\n",
    "        self.generator = Generator(noise_size, num_filters=num_filters, num_blocks=num_generator_blocks,\n",
    "                                   start_image_size=4, num_channels=num_channels)\n",
    "        \n",
    "        if discriminator_type == \"Image\":\n",
    "            self.discriminator = ImageDiscriminator(num_filters=num_filters, num_blocks=num_discriminator_blocks, \n",
    "                                                    num_channels=num_channels)\n",
    "        elif discriminator_type == \"Vector\":\n",
    "            self.discriminator = VectorDiscriminator(input_size=embedding_size, \n",
    "                                                     hidden_size=discriminator_hidden_size,\n",
    "                                                     num_blocks=num_discriminator_blocks)\n",
    "        else:\n",
    "            raise ValueError(\"Discriminator type not recognized.\")\n",
    "        self.discriminator_epsilon = discriminator_epsilon\n",
    "        \n",
    "        # Assumption that the dual space is the same as the original space.\n",
    "        self.gamma = gamma\n",
    "        self.lambda_penalty = gamma\n",
    "        \n",
    "        self.register_buffer(\"penalty_grad_outputs\", torch.ones(batch_size))\n",
    "        self.register_buffer(\"noise_buffer\", torch.ones((batch_size, noise_size)))\n",
    "        self.register_buffer(\"epsilon_buffer\", torch.ones(batch_size, 1, 1, 1))\n",
    "        \n",
    "    def forward_train_generator(self, noise=None):\n",
    "        generated_image = self.forward_predict_generator(noise)\n",
    "        discriminator_score_generated = self.forward_predict_discriminator(generated_image)\n",
    "        return self.generator_loss(discriminator_score_generated)\n",
    "    \n",
    "    def forward_train_discriminator(self, real_images, noise=None):\n",
    "        generated_images = self.forward_predict_generator(noise)\n",
    "        discriminator_score_generated = self.forward_predict_discriminator(generated_images)\n",
    "        discriminator_score_real = self.forward_predict_discriminator(real_images)\n",
    "        return self.discriminator_loss(discriminator_score_real, discriminator_score_generated, \n",
    "                                       real_images, generated_images)\n",
    "        \n",
    "    def forward_predict_generator(self, noise=None):\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise()\n",
    "        return self.generator(noise)\n",
    "        \n",
    "    def forward_predict_discriminator(self, images):\n",
    "        return self.discriminator(images)\n",
    "    \n",
    "    def generate_noise(self):\n",
    "        return torch.randn_like(self.noise_buffer)\n",
    "    \n",
    "    def generator_loss(self, d_score_generated):\n",
    "        return torch.mean(d_score_generated) / self.gamma \n",
    "    \n",
    "    def stable_norm(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        alpha, _ = (x.abs() + 1e-5).max(1)        \n",
    "        return alpha * (x / alpha).norm(p=2, dim=1)\n",
    "    \n",
    "    def gradient_penalty(self, real_fake_mix):\n",
    "        d_score_mix = self.discriminator(real_fake_mix).squeeze(1)\n",
    "        \n",
    "        gradients = torch.autograd.grad(d_score_mix, real_fake_mix, grad_outputs=self.penalty_grad_outputs,\n",
    "                                        create_graph=True)[0]\n",
    "        gradient_penalty = self.lambda_penalty * torch.mean(self.stable_norm(gradients) / gamma - 1) ** 2\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def discriminator_loss(self, d_score_real, d_score_generated, real_images, generated_images):\n",
    "        wasserstein_loss = (torch.mean(d_score_generated) - torch.mean(d_score_real)) / self.gamma\n",
    "        \n",
    "        epsilon = self.epsilon_buffer.uniform_(0, 1)\n",
    "        real_fake_mix = epsilon * generated_images + (1 - epsilon) * real_images \n",
    "        gradient_penalty = self.gradient_penalty(real_fake_mix)   \n",
    "        \n",
    "        d_regularizer_mean = torch.mean(d_score_real ** 2)\n",
    "        \n",
    "#         print(\"w\")\n",
    "#         print(-wasserstein_loss)\n",
    "#         print(\"g\")\n",
    "#         print(gradient_penalty)\n",
    "#         print(\"d\")\n",
    "#         print(self.discriminator_epsilon * d_regularizer_mean)\n",
    "        d_loss = -wasserstein_loss + gradient_penalty + self.discriminator_epsilon * d_regularizer_mean\n",
    "        return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:42:57.218380Z",
     "start_time": "2019-01-28T06:42:57.206362Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BanachWGAN(nn.Module):\n",
    "    def __init__(self, gamma, noise_size=128, num_filters=128, num_generator_blocks=3, num_discriminator_blocks=4,\n",
    "                 batch_size=64, num_channels=3, discriminator_epsilon=1e-5, embedding_func=identity_embedding, \n",
    "                 embedding_size=None, discriminator_hidden_size=None, discriminator_type=\"Image\"):  \n",
    "        super(BanachWGAN, self).__init__(gamma=gamma, noise_size=noise_size, num_filters=num_filters, \n",
    "                                         num_generator_blocks=num_generator_blocks, \n",
    "                                         num_discriminator_blocks=num_discriminator_blocks, \n",
    "                                         batch_size=batch_size, num_channels=num_channels, \n",
    "                                         discriminator_epsilon=discriminator_epsilon,\n",
    "                                         embedding_size=None, discriminator_hidden_size=None, \n",
    "                                         discriminator_type=\"Image\")\n",
    "        \n",
    "        self.embedding_func = embedding_func\n",
    "        \n",
    "        if discriminator_type == \"Vector\":\n",
    "            self.noise_buffer.squeeze(-1)\n",
    "            self.noise_buffer.squeeze(-1)\n",
    "    \n",
    "    def forward_train_discriminator(self, real_images, noise=None):\n",
    "        generated_images = self.forward_predict_generator(noise)\n",
    "        real_data = self.embedding_func(real_images)\n",
    "        generated_data = self.embedding_func(generated_images)\n",
    "        \n",
    "        discriminator_score_generated = self.discriminator(generated_data)\n",
    "        discriminator_score_real = self.discriminator(real_data)\n",
    "        \n",
    "        return self.discriminator_loss(discriminator_score_real, discriminator_score_generated, \n",
    "                                       real_data, generated_data)\n",
    "        \n",
    "    def forward_predict_discriminator(self, images):\n",
    "        return self.discriminator(self.embedding_func(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:42:58.703705Z",
     "start_time": "2019-01-28T06:42:58.694152Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MetricWGAN(WGAN):\n",
    "    def __init__(self, gamma, noise_size=128, num_filters=128, num_generator_blocks=3, num_discriminator_blocks=4,\n",
    "                 batch_size=64, num_channels=3, discriminator_epsilon=1e-5, embedding_func=identity_embedding, \n",
    "                 embedding_size=None, discriminator_hidden_size=None, discriminator_type=\"Image\"):  \n",
    "        super(MetricWGAN, self).__init__(gamma=gamma, noise_size=noise_size, num_filters=num_filters, \n",
    "                                         num_generator_blocks=num_generator_blocks, \n",
    "                                         num_discriminator_blocks=num_discriminator_blocks, \n",
    "                                         batch_size=batch_size, num_channels=num_channels, \n",
    "                                         discriminator_epsilon=discriminator_epsilon,\n",
    "                                         embedding_size=None, discriminator_hidden_size=None, \n",
    "                                         discriminator_type=\"Image\")\n",
    "        self.embedding_func = embedding_func\n",
    "        \n",
    "        self.noise_buffer.unsqueeze(1)\n",
    "        self.noise_buffer.repeat(2, 1, 1, 1, 1).view(2 * batch_size, 1, 1, 1)\n",
    "        self.batch_size = batch_size\n",
    "        self.pdist = nn.PairwiseDistance()\n",
    "    \n",
    "    def gradient_penalty(self, real_fake_mix):\n",
    "        sampled_data = self.embedding_func(real_fake_mix) # 2 * batch_size x embedding_size\n",
    "        sampled_scores = self.discriminator(sampled_data).view(2, self.batch_size) # 2 x batch_size\n",
    "        sampled_data = sampled_data.view(2, self.batch_size, -1) # 2 x batch_size x embedding_size\n",
    "        \n",
    "        num_dist = (sampled_scores[0] - sampled_scores[1]).abs()\n",
    "        denom_dist = self.pdist(sampled_data[0], sampled_data[1])\n",
    "        \n",
    "        return ((num_dist/denom_dist - 1)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:48:54.856111Z",
     "start_time": "2019-01-28T06:48:54.843177Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "# There is one discrepancy in this training code and the bwgan github implementation. That\n",
    "# version uses an exponential moving average of the weights during evaluation.\n",
    "#\n",
    "# One other discrepancy with bwgan is the lack of usage of warm restarts for SGD. That's mentioned in\n",
    "# the paper but I could not see in the implementation.\n",
    "#\n",
    "# The last main discrepancy is related to the model. In the bwgan code gamma is computed each batch. Here\n",
    "# we compute gamma over the dataset instead. The difference should be very minor as gamma's value across batches\n",
    "# is pretty stable. For MNIST gamma appeared to range from 29.8-30.1 from looking at a dozen gamma values.\n",
    "def gan_train(model, dset_loader, optimizers, lr_schedulers, num_updates=1e5,\n",
    "              use_cuda=False, num_discriminator=5):\n",
    "    steps_so_far = 0\n",
    "    curr_epoch = 0\n",
    "    \n",
    "    discriminator_optimizer, generator_optimizer = optimizers\n",
    "    discriminator_lr_scheduler, generator_lr_scheduler = lr_schedulers\n",
    "        \n",
    "    while True:\n",
    "        print('Epoch {} - Step {}/{}'.format(curr_epoch, steps_so_far, num_updates))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Iterate over data.\n",
    "        for data, _ in dset_loader:\n",
    "            if steps_so_far % 1000 == 0:\n",
    "                print(steps_so_far)\n",
    "            \n",
    "            if steps_so_far >= num_updates:\n",
    "                return model\n",
    "            \n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            \n",
    "            loss = model.forward_train_discriminator(data)\n",
    "            loss.backward()\n",
    "            \n",
    "            if steps_so_far % 100 == 0:\n",
    "                print(\"Discriminator Loss\")\n",
    "                print(loss.item())\n",
    "            \n",
    "            discriminator_optimizer.step()\n",
    "            # zero the parameter gradients\n",
    "            discriminator_optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "            if steps_so_far % num_discriminator == num_discriminator-1:\n",
    "                # This is done because in 1 step the discriminator sees two batches of images\n",
    "                # while the generator only sees 1. I don't think this matters and will eventually\n",
    "                # drop it if I can get the model to work.\n",
    "                loss = (model.forward_train_generator() + model.forward_train_generator())/2\n",
    "                loss.backward()\n",
    "                generator_optimizer.step()\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                generator_optimizer.zero_grad()\n",
    "\n",
    "                discriminator_lr_scheduler.step()\n",
    "                generator_lr_scheduler.step()\n",
    "                    \n",
    "            if steps_so_far % 500 == 0:\n",
    "                generated_images = model.forward_predict_generator()\n",
    "                first_image = generated_images[0, 0].cpu().detach().numpy()\n",
    "                min_val = float(np.amin(first_image))\n",
    "                max_val = float(np.amax(first_image))\n",
    "                plt.title(\"Generated image, range {} to {}\".format(round(min_val, 3), round(max_val, 3)))\n",
    "                plt.imshow(first_image)\n",
    "                plt.show()\n",
    "            \n",
    "            steps_so_far += 1\n",
    "        curr_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:43:04.019994Z",
     "start_time": "2019-01-28T06:43:04.014679Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_gamma(dset_loader):\n",
    "    num_images = len(dset_loader.dataset)\n",
    "    gamma = 0.0\n",
    "    \n",
    "    for data, _ in dset_loader:\n",
    "        batch_size = data.size()[0]\n",
    "        gamma += data.cuda().view(batch_size, -1).norm(2, dim=1).sum().item() / num_images\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T07:12:38.431430Z",
     "start_time": "2019-01-28T07:12:38.426616Z"
    }
   },
   "outputs": [],
   "source": [
    "noise_size = 128\n",
    "batch_size = 1\n",
    "base_lr = 2e-4\n",
    "num_updates = int(1e4)\n",
    "num_discriminator = 5\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T06:43:06.783237Z",
     "start_time": "2019-01-28T06:43:06.775577Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_data(name, train=True):\n",
    "    assert name in [\"mnist\", \"cifar\", \"celeba\"]\n",
    "    transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "    if name == \"mnist\":\n",
    "        return datasets.MNIST(\"mnist\", train=train, download=True, transform=transform)\n",
    "    if name == \"cifar\":\n",
    "        return datasets.CIFAR10(\"cifar\", train=train, transform=transform, download=True)\n",
    "    if name == \"celeba\":\n",
    "        if train == True:\n",
    "            dset_str = \"train\"\n",
    "        else:\n",
    "            dset_str = \"test\"\n",
    "        with open(\"celeba_64_bgr_-1_to_1_%s.pkl\" % dset_str, \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T07:13:37.214775Z",
     "start_time": "2019-01-28T07:12:39.789305Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset = get_data(\"cifar\")\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                   num_workers=4, pin_memory=True, drop_last=True)\n",
    "gamma = compute_gamma(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-28T07:15:31.348Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "model = WGAN(gamma=gamma, noise_size=noise_size, batch_size=batch_size, num_channels=num_channels)\n",
    "\n",
    "# if use_cuda:\n",
    "#     model = model.cuda()\n",
    "#     model.discriminator = nn.DataParallel(model.discriminator)\n",
    "#     model.generator = nn.DataParallel(model.generator)\n",
    "\n",
    "discriminator_optimizer = optim.Adam(model.discriminator.parameters(), betas=(0, 0.9), lr=base_lr)\n",
    "generator_optimizer = optim.Adam(model.generator.parameters(), betas=(0, 0.9), lr=base_lr)\n",
    "discriminator_lr_scheduler = optim.lr_scheduler.LambdaLR(discriminator_optimizer, lambda step: max(0, (1 - step/num_updates)))\n",
    "generator_lr_scheduler = optim.lr_scheduler.LambdaLR(generator_optimizer, lambda step: max(0, (1 - step/num_updates)))\n",
    "optimizers = discriminator_optimizer, generator_optimizer\n",
    "lr_schedulers = discriminator_lr_scheduler, generator_lr_scheduler\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = gan_train(model, train_dataloader, optimizers, lr_schedulers, num_updates=num_updates, \n",
    "                  use_cuda=use_cuda, num_discriminator=num_discriminator)\n",
    "print(time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
