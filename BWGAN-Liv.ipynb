{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdd28a3c090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from graphviz import Digraph\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "global py_conv_vec\n",
    "global act_vec\n",
    "global dense_output\n",
    "global upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     3,
     20,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def upsample(x):\n",
    "    return nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "def downsample(x):\n",
    "    return nn.functional.interpolate(x, scale_factor=0.5, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "def conv_layer(in_filters, out_filters=32, kernel_size=3, he_init=True):\n",
    "    same_padding = (kernel_size-1)//2\n",
    "    conv = nn.Conv2d(in_filters, out_filters, kernel_size=kernel_size, padding=same_padding)\n",
    "    \n",
    "    if he_init:\n",
    "        he_init_constant = math.sqrt(6 / (in_filters * kernel_size**2))\n",
    "        nn.init.uniform_(conv.weight, -he_init_constant, he_init_constant)\n",
    "    else:\n",
    "        xavier_init_constant = math.sqrt(6 / ((in_filters + out_filters) * kernel_size**2))\n",
    "        nn.init.uniform_(conv.weight, -xavier_init_constant, xavier_init_constant)\n",
    "    nn.init.constant_(conv.bias, 0)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def bn(channels):\n",
    "    batchnorm = nn.BatchNorm2d(channels, eps=1e-5)\n",
    "    nn.init.constant_(batchnorm.weight, 1)\n",
    "    return batchnorm\n",
    "\n",
    "def linear(in_features, out_features):\n",
    "    linear_layer = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    xavier_init_constant = math.sqrt(6/(in_features+out_features))\n",
    "    nn.init.uniform_(linear_layer.weight, -xavier_init_constant, xavier_init_constant)\n",
    "    nn.init.constant_(linear_layer.bias, 0)\n",
    "    \n",
    "    return linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     55
    ]
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, resample=None, normalize=False, activation=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.in_filters = in_filters\n",
    "        self.out_filters = out_filters\n",
    "        self.resample = resample\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.conv1 = conv_layer(in_filters, out_filters)\n",
    "        self.conv2 = conv_layer(out_filters, out_filters)\n",
    "        \n",
    "        if resample:\n",
    "            self.conv3 = conv_layer(in_filters, out_filters, kernel_size=1, he_init=False)\n",
    "        \n",
    "        if normalize:\n",
    "            self.bn1 = bn(in_filters)\n",
    "            self.bn2 = bn(out_filters)\n",
    "            \n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "                \n",
    "    def forward(self, x): \n",
    "        orig_input = x\n",
    "        \n",
    "#         print(\"ACTUAL BATCHNORM\")\n",
    "#         print(x)\n",
    "#         print(x.size())\n",
    "        \n",
    "        \n",
    "        if self.normalize:\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "        \n",
    "#         print(\"ACTUAL BATCHNORM RESULT\")\n",
    "        \n",
    "#         print(x)\n",
    "#         print(x.size())\n",
    "#         raise ValueError(\"hi\")\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        print(\"Activated\")\n",
    "        print(x[0, :2, :2,:2])\n",
    "        global act_vec\n",
    "        act_vec = x\n",
    "        \n",
    "        if self.resample == 'up':\n",
    "            x = upsample(x)\n",
    "            \n",
    "        print(\"Upsampled\")\n",
    "        print(x[0, :2, :2,:2])\n",
    "        global upsampled\n",
    "        upsampled = x   \n",
    "       \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        print(\"AFTER conv1\")\n",
    "        print(x[0, :2, :2,:2])\n",
    "        global py_conv_vec\n",
    "        py_conv_vec = x\n",
    "        \n",
    "        print(\"Conv weights\")\n",
    "        print(self.conv1.weight.data[0, :3, :3, :3])\n",
    "        print(\"------\")\n",
    "        print(self.conv1.bias.data)\n",
    "        \n",
    "        \n",
    "        raise ValueError(\"hi\")\n",
    "\n",
    "        \n",
    "        if self.normalize:\n",
    "            x = self.bn2(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        if self.resample == 'down':\n",
    "            x = downsample(x)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.resample == 'down': \n",
    "            shortcut_x = downsample(self.conv3(orig_input))\n",
    "        elif self.resample == 'up':\n",
    "            shortcut_x = self.conv3(upsample(orig_input))\n",
    "        elif self.resample == None:\n",
    "            shortcut_x = orig_input\n",
    "        \n",
    "        return x + shortcut_x\n",
    "    \n",
    "class SmallResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, activation=None):\n",
    "        super(SmallResBlock, self).__init__()\n",
    "        self.in_filters = in_filters\n",
    "        self.out_filters = out_filters\n",
    "        \n",
    "        self.conv1 = conv_layer(in_filters, out_filters)\n",
    "        self.conv2 = conv_layer(out_filters, out_filters)\n",
    "        self.conv3 = conv_layer(in_filters, out_filters, kernel_size=1, he_init=False)\n",
    "            \n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "                \n",
    "    def forward(self, x): \n",
    "        orig_input = x\n",
    "       \n",
    "        x = self.conv1(x)   \n",
    "        x = downsample(self.conv2(self.activation(x)))\n",
    "\n",
    "        # Shortcut\n",
    "        shortcut_x = self.conv3(downsample(orig_input))\n",
    "        \n",
    "        return x + shortcut_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, num_filters=128, num_blocks=3, start_image_size=4, num_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.num_filters = num_filters\n",
    "        self.start_image_size = start_image_size\n",
    "        \n",
    "        self.first_linear = linear(input_size, num_filters * start_image_size ** 2)\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        for _ in range(num_blocks):\n",
    "            self.resblocks.append(\n",
    "                ResBlock(in_filters=self.num_filters, \n",
    "                         out_filters=self.num_filters, \n",
    "                         resample='up', \n",
    "                         normalize=True))\n",
    "            \n",
    "        self.last_layer = conv_layer(num_filters, num_channels)\n",
    "        self.bn = bn(num_filters)\n",
    "        self.manually_initialize()\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        \n",
    "        print(\"Should be all ones\")\n",
    "        print(noise)\n",
    "        print(noise.size())\n",
    "        \n",
    "        x = self.first_linear(noise)\n",
    "        \n",
    "        print(\"Literally just dense\")\n",
    "        \n",
    "#         print(\"A1\")\n",
    "        \n",
    "        print(x)\n",
    "        print(x.size())\n",
    "        \n",
    "#         x = x.view(-1, self.num_filters, self.start_image_size, self.start_image_size)\n",
    "        x = x.view(-1, self.start_image_size, self.start_image_size, self.num_filters)\n",
    "        print(\"first view reshape\", x[0, :3, :3, :3])\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        print(\"second view reshape\", x[0, :3, :3, :3])\n",
    "        \n",
    "        \n",
    "        global dense_output\n",
    "        dense_output = x\n",
    "        \n",
    "        print(\"After Dense\")\n",
    "        print(x.permute(0,2,3,1))\n",
    "        print(x.size())\n",
    "        \n",
    "        for resblock in self.resblocks:\n",
    "            x = resblock(x)\n",
    "#             print(\"B\")\n",
    "#             print(x)\n",
    "#             print(x.size())\n",
    "            \n",
    "        x = self.activation(self.bn(x))\n",
    "        result = self.last_layer(x)\n",
    "        return torch.tanh(result)\n",
    "    \n",
    "    def manually_initialize(self):\n",
    "        print(self.first_linear.weight.data[0,0:3])\n",
    "        self.first_linear.weight.data = torch.tensor(np.load(\"params/generator_dense_kernel:0.npy\")).transpose(0,1)\n",
    "        self.first_linear.bias.data = torch.tensor(np.load(\"params/generator_dense_bias:0.npy\"))\n",
    "        print(self.first_linear.weight.data[0:3,0])\n",
    "        \n",
    "        self.resblocks[0].bn1.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_gamma:0.npy\"))\n",
    "        self.resblocks[0].bn1.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].bn2.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_1_gamma:0.npy\"))\n",
    "        self.resblocks[0].bn2.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_1_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].bn1.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_2_gamma:0.npy\"))\n",
    "        self.resblocks[1].bn1.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_2_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].bn2.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_3_gamma:0.npy\"))\n",
    "        self.resblocks[1].bn2.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_3_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].bn1.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_4_gamma:0.npy\"))\n",
    "        self.resblocks[2].bn1.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_4_beta:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].bn2.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_5_gamma:0.npy\"))\n",
    "        self.resblocks[2].bn2.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_5_beta:0.npy\"))\n",
    "        \n",
    "        self.bn.weight.data = torch.tensor(np.load(\"params/generator_BatchNorm_6_gamma:0.npy\"))\n",
    "        self.bn.bias.data = torch.tensor(np.load(\"params/generator_BatchNorm_6_beta:0.npy\"))\n",
    "        \n",
    "        # Insert \"contiguous\" to make conv layers work correctly.  Not strictly necessary when using the gpu,\n",
    "        # but will hopefully help us avoid headaches later\n",
    "        self.resblocks[0].conv1.weight.data = torch.tensor(np.load(\"params/generator_conv2d_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv1.bias.data = torch.tensor(np.load(\"params/generator_conv2d_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv2.weight.data = torch.tensor(np.load(\"params/generator_conv2d_1_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv2.bias.data = torch.tensor(np.load(\"params/generator_conv2d_1_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[0].conv3.weight.data = torch.tensor(np.load(\"params/generator_conv2d_2_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[0].conv3.bias.data = torch.tensor(np.load(\"params/generator_conv2d_2_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv1.weight.data = torch.tensor(np.load(\"params/generator_conv2d_3_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv1.bias.data = torch.tensor(np.load(\"params/generator_conv2d_3_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv2.weight.data = torch.tensor(np.load(\"params/generator_conv2d_4_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv2.bias.data = torch.tensor(np.load(\"params/generator_conv2d_4_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[1].conv3.weight.data = torch.tensor(np.load(\"params/generator_conv2d_5_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[1].conv3.bias.data = torch.tensor(np.load(\"params/generator_conv2d_5_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv1.weight.data = torch.tensor(np.load(\"params/generator_conv2d_6_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv1.bias.data = torch.tensor(np.load(\"params/generator_conv2d_6_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv2.weight.data = torch.tensor(np.load(\"params/generator_conv2d_7_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv2.bias.data = torch.tensor(np.load(\"params/generator_conv2d_7_bias:0.npy\"))\n",
    "        \n",
    "        self.resblocks[2].conv3.weight.data = torch.tensor(np.load(\"params/generator_conv2d_8_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.resblocks[2].conv3.bias.data = torch.tensor(np.load(\"params/generator_conv2d_8_bias:0.npy\"))\n",
    "        \n",
    "        self.last_layer.weight.data = torch.tensor(np.load(\"params/generator_conv2d_9_kernel:0.npy\")).permute(3,2,1,0).contiguous()\n",
    "        self.last_layer.bias.data = torch.tensor(np.load(\"params/generator_conv2d_9_bias:0.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.block = nn.Sequential(linear(input_size, output_size), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class VectorDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_blocks):\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.blocks.append(MLPLayer(input_size, hidden_size))\n",
    "        \n",
    "        for _ in range(num_blocks - 1):\n",
    "            self.blocks.append(hidden_size, hidden_size)\n",
    "        \n",
    "        self.blocks.append(linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ImageDiscriminator(nn.Module):\n",
    "    def __init__(self, num_filters=128, num_blocks=4, num_channels=3):\n",
    "        super(ImageDiscriminator, self).__init__()\n",
    "        assert num_blocks >= 2, \"Number of conv layers in the discriminator must be >= 2.\"\n",
    "        \n",
    "        self.resblocks = nn.ModuleList()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.resblocks.append(SmallResBlock(in_filters=num_channels, \n",
    "                                            out_filters=num_filters))\n",
    "        self.resblocks.append(ResBlock(in_filters=num_filters, \n",
    "                                       out_filters=num_filters, \n",
    "                                       resample='down'))\n",
    "        for _ in range(num_blocks - 2):\n",
    "            self.resblocks.append(ResBlock(in_filters=num_filters, \n",
    "                                           out_filters=num_filters))\n",
    "            \n",
    "        self.last_linear = linear(num_filters, 1)\n",
    "        self.manually_initialize()\n",
    "   \n",
    "    def forward(self, x):\n",
    "        for resblock in self.resblocks:\n",
    "            x = resblock(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        x = x.mean(dim=(-1,-2))\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "    \n",
    "    def manually_initialize(self):\n",
    "        print('hi')\n",
    "#         self.last_linear.weight.data = torch.tensor(np.load(\"params/discriminator_dense_kernel:0.npy\")).transpose(0,1)\n",
    "#         self.last_linear.bias.data = torch.tensor(np.load(\"params/discriminator_dense_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[0].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[0].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[0].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_1_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[0].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_1_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[0].conv3.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_2_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[0].conv3.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_2_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[1].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_3_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[1].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_3_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[1].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_4_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[1].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_4_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[1].conv3.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_5_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[1].conv3.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_5_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[2].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_6_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[2].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_6_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[2].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_7_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[2].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_7_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[3].conv1.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_8_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[3].conv1.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_8_bias:0.npy\"))\n",
    "        \n",
    "#         self.resblocks[3].conv2.weight.data = torch.tensor(np.load(\"params/discriminator_conv2d_9_kernel:0.npy\")).permute(3,2,1,0)\n",
    "#         self.resblocks[3].conv2.bias.data = torch.tensor(np.load(\"params/discriminator_conv2d_9_bias:0.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def identity_embedding(pic):\n",
    "    return pic\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, gamma, noise_size=128, num_filters=128, num_generator_blocks=3, num_discriminator_blocks=4,\n",
    "                 batch_size=64, num_channels=3, discriminator_epsilon=1e-5, banach=True, \n",
    "                 embedding_func=identity_embedding, embedding_size=None, discriminator_hidden_size=None,\n",
    "                 discriminator_type=\"Image\"):  \n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = Generator(noise_size, num_filters=num_filters, num_blocks=num_generator_blocks,\n",
    "                                   start_image_size=4, num_channels=num_channels)\n",
    "        \n",
    "        if discriminator_type == \"Image\":\n",
    "            self.discriminator = ImageDiscriminator(num_filters=num_filters, num_blocks=num_discriminator_blocks, \n",
    "                                                    num_channels=num_channels)\n",
    "        elif discriminator_type == \"Vector\":\n",
    "            self.discriminator = VectorDiscriminator(input_size=embedding_size, \n",
    "                                                     hidden_size=discriminator_hidden_size,\n",
    "                                                     num_blocks=num_discriminator_blocks)\n",
    "        else:\n",
    "            raise ValueError(\"Discriminator type not recognized.\")\n",
    "        self.discriminator_epsilon = discriminator_epsilon\n",
    "        \n",
    "        # Assumption that the dual space is the same as the original space.\n",
    "        self.gamma = gamma\n",
    "        self.lambda_penalty = gamma\n",
    "        \n",
    "        self.banach = banach # Boolean parameter that decides on whether to do a banach/metric space based wgan.\n",
    "        self.embedding_func = embedding_func # The embedding function used.\n",
    "        \n",
    "        self.register_buffer(\"penalty_grad_outputs\", torch.ones(batch_size))\n",
    "        self.register_buffer(\"noise_buffer\", torch.ones((batch_size, noise_size)))\n",
    "        self.register_buffer(\"epsilon_buffer\", torch.ones(batch_size, 1, 1, 1))\n",
    "        \n",
    "    def forward_train_generator(self, noise=None):\n",
    "        generated_image = self.forward_predict_generator(noise)\n",
    "        discriminator_score_generated = self.forward_predict_discriminator(generated_image)\n",
    "        return self.generator_loss(discriminator_score_generated)\n",
    "    \n",
    "    def forward_train_discriminator(self, real_images, noise=None):\n",
    "        generated_images = self.forward_predict_generator(noise)\n",
    "        discriminator_score_generated = self.forward_predict_discriminator(generated_images)\n",
    "        discriminator_score_real = self.forward_predict_discriminator(real_images)\n",
    "        return self.discriminator_loss(discriminator_score_real, discriminator_score_generated, real_images, generated_images)\n",
    "        \n",
    "    def forward_predict_generator(self, noise=None):\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise()\n",
    "        return self.generator(noise)\n",
    "        \n",
    "    def forward_predict_discriminator(self, image):\n",
    "        return self.discriminator(image)\n",
    "    \n",
    "    def generate_noise(self):\n",
    "        return torch.randn_like(self.noise_buffer)\n",
    "    \n",
    "    def generator_loss(self, d_score_generated):\n",
    "#         print(\"Generator loss\", torch.mean(d_generated_train) / self.gamma)\n",
    "        return torch.mean(d_score_generated) / self.gamma #NOTE: Mehdi's version had a negative sign, original was positive\n",
    "    \n",
    "    def stable_norm(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        alpha, _ = (x.abs() + 1e-5).max(1)\n",
    "        \n",
    "        return alpha * (x/alpha.unsqueeze(1)).norm(p=2, dim=1)\n",
    "    \n",
    "    def discriminator_loss(self, d_score_real, d_score_generated, real_images, generated_images):\n",
    "#         print(\"d score generated\", torch.mean(d_score_generated))\n",
    "#         print(\"d score real\", torch.mean(d_score_real))\n",
    "        wasserstein_loss = (torch.mean(d_score_generated) - torch.mean(d_score_real)) / self.gamma\n",
    "#         print(\"wass loss\", wasserstein_loss)\n",
    "        epsilon = self.epsilon_buffer.uniform_(0, 1)\n",
    "        real_fake_mix = epsilon * generated_images + (1 - epsilon) * real_images \n",
    "        d_score_mix = self.discriminator(real_fake_mix).squeeze(1)\n",
    "        \n",
    "        gradients = torch.autograd.grad(d_score_mix, real_fake_mix, grad_outputs=self.penalty_grad_outputs,\n",
    "                                        create_graph=True)[0]\n",
    "        \n",
    "#         print(gradients)\n",
    "        gradient_penalty = torch.mean(self.stable_norm(gradients) / gamma - 1) ** 2\n",
    "        print(\"grad penalty\", float(self.lambda_penalty * gradient_penalty))\n",
    "        d_regularizer_mean = torch.mean(d_score_real ** 2)\n",
    "#         print(\"regularizer mean\", float(self.discriminator_epsilon * d_regularizer_mean))\n",
    "#         \n",
    "        #NOTE: Mehdi's version had the wassestein loss positive, original seems to be negative\n",
    "#         print(\"w loss is\", -float(wasserstein_loss))\n",
    "        d_loss = -wasserstein_loss + self.lambda_penalty * gradient_penalty + self.discriminator_epsilon * d_regularizer_mean\n",
    "#         print(\"Overall d_loss\", d_loss.item())\n",
    "        return d_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# There is one discrepancy in this training code and the bwgan github implementation. That\n",
    "# version uses an exponential moving average of the weights during evaluation.\n",
    "#\n",
    "# One other discrepancy with bwgan is the lack of usage of warm restarts for SGD. That's mentioned in\n",
    "# the paper but I could not see in the implementation.\n",
    "#\n",
    "# The last main discrepancy is related to the model. In the bwgan code gamma is computed each batch. Here\n",
    "# we compute gamma over the dataset instead. The difference should be very minor as gamma's value across batches\n",
    "# is pretty stable. For MNIST gamma appeared to range from 29.8-30.1 from looking at a dozen gamma values.\n",
    "def gan_train(model, dset_loader, optimizers, lr_schedulers, num_updates=1e5,\n",
    "              use_cuda=False, num_discriminator=5):\n",
    "    steps_so_far = 0\n",
    "    curr_epoch = 0\n",
    "    \n",
    "    discriminator_optimizer, generator_optimizer = optimizers\n",
    "    discriminator_lr_scheduler, generator_lr_scheduler = lr_schedulers\n",
    "    \n",
    "    display_generator = False\n",
    "    \n",
    "    while True:\n",
    "        print('Epoch {} - Step {}/{}'.format(curr_epoch, steps_so_far, num_updates))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Iterate over data.\n",
    "        for data, _ in dset_loader:\n",
    "            \n",
    "            print(steps_so_far)\n",
    "            if steps_so_far % 1000 == 0:\n",
    "                print(steps_so_far)\n",
    "            \n",
    "            if steps_so_far >= num_updates:\n",
    "                return model\n",
    "            \n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            \n",
    "            loss = model.forward_train_discriminator(data)\n",
    "#             register_hooks(loss)\n",
    "            \n",
    "            if steps_so_far % 100 == 4:\n",
    "                print(\"Discriminator Loss: \", loss.item())\n",
    "                display_generator = True\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            discriminator_optimizer.step()\n",
    "            # zero the parameter gradients\n",
    "            discriminator_optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "            if steps_so_far % num_discriminator == num_discriminator-1:\n",
    "                loss = model.forward_train_generator()\n",
    "                loss.backward()\n",
    "                generator_optimizer.step()\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                generator_optimizer.zero_grad()\n",
    "\n",
    "                discriminator_lr_scheduler.step()\n",
    "                generator_lr_scheduler.step()\n",
    "                if display_generator:\n",
    "                    print(\"Generator Loss: \", loss.item())\n",
    "                    display_generator = False\n",
    "                    \n",
    "                    \n",
    "            if steps_so_far % 500 == 0:\n",
    "                generated_images = model.forward_predict_generator()\n",
    "                first_image = generated_images[0, 0].cpu().detach().numpy()\n",
    "                min_val = float(np.amin(first_image))\n",
    "                max_val = float(np.amax(first_image))\n",
    "                plt.title(\"Generated image, range {} to {}\".format(round(min_val, 3), round(max_val, 3)))\n",
    "                plt.imshow(first_image)\n",
    "                plt.show()\n",
    "            \n",
    "            steps_so_far += 1\n",
    "        \n",
    "        curr_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_gamma(dset_loader):\n",
    "    num_images = len(dset_loader.dataset)\n",
    "    gamma = 0.0\n",
    "    \n",
    "    for data, _ in dset_loader:\n",
    "        batch_size = data.size()[0]\n",
    "        gamma += data.cuda().view(batch_size, -1).norm(2, dim=1).sum().item() / num_images\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "            return False\n",
    "        \n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e4).any()\n",
    "    \n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            for grad in grad_output:\n",
    "                if is_bad_grad(grad):\n",
    "                    print(fn)\n",
    "#                     raise ValueError(\"Hi\")\n",
    "        fn.register_hook(register_grad)\n",
    "    iter_graph(var.grad_fn, hook_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 128\n",
    "batch_size = 1\n",
    "use_cuda = True\n",
    "base_lr = 2e-4\n",
    "num_updates = int(1e2)\n",
    "num_discriminator = 5\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name, train=True):\n",
    "    assert name in [\"mnist\", \"cifar\", \"celeba\"]\n",
    "    transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "    if name == \"mnist\":\n",
    "        return datasets.MNIST(\"mnist\", train=train, download=True, transform=transform)\n",
    "    if name == \"cifar\":\n",
    "        return datasets.CIFAR10(\"cifar\", train=train, transform=transform, download=True)\n",
    "    if name == \"celeba\":\n",
    "        if train == True:\n",
    "            dset_str = \"train\"\n",
    "        else:\n",
    "            dset_str = \"test\"\n",
    "        with open(\"celeba_64_bgr_-1_to_1_%s.pkl\" % dset_str, \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = get_data(\"cifar\")\n",
    "# train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "#                                    num_workers=4, pin_memory=True, drop_last=True)\n",
    "gamma = 27 #compute_gamma(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0374, -0.0036,  0.0143])\n",
      "tensor([-0.0185,  0.0220,  0.0163])\n",
      "hi\n",
      "Should be all ones\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n",
      "torch.Size([1, 128])\n",
      "Literally just dense\n",
      "tensor([[-0.2824,  0.1309, -0.1149,  ..., -0.3147, -0.2160, -0.0382]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "torch.Size([1, 2048])\n",
      "first view reshape tensor([[[-0.2824,  0.1309, -0.1149],\n",
      "         [ 0.1462,  0.3888, -0.3026],\n",
      "         [ 0.3701,  0.1530,  0.4670]],\n",
      "\n",
      "        [[-0.4724, -0.7412, -0.2681],\n",
      "         [ 1.0154,  0.1076, -0.5858],\n",
      "         [ 0.3558, -0.1052,  0.4186]],\n",
      "\n",
      "        [[ 0.2085, -0.8711,  0.7487],\n",
      "         [-0.3292, -0.2729,  0.5168],\n",
      "         [-0.7029,  0.5389,  0.2108]]], grad_fn=<SliceBackward>)\n",
      "second view reshape tensor([[[-0.2824,  0.1462,  0.3701],\n",
      "         [-0.4724,  1.0154,  0.3558],\n",
      "         [ 0.2085, -0.3292, -0.7029]],\n",
      "\n",
      "        [[ 0.1309,  0.3888,  0.1530],\n",
      "         [-0.7412,  0.1076, -0.1052],\n",
      "         [-0.8711, -0.2729,  0.5389]],\n",
      "\n",
      "        [[-0.1149, -0.3026,  0.4670],\n",
      "         [-0.2681, -0.5858,  0.4186],\n",
      "         [ 0.7487,  0.5168,  0.2108]]], grad_fn=<SliceBackward>)\n",
      "After Dense\n",
      "tensor([[[[-0.2824,  0.1309, -0.1149,  ..., -0.2591,  0.2442, -0.5092],\n",
      "          [ 0.1462,  0.3888, -0.3026,  ..., -0.6472, -0.5298,  0.4293],\n",
      "          [ 0.3701,  0.1530,  0.4670,  ..., -0.1710, -0.1390,  0.4779],\n",
      "          [ 0.0865,  0.0982,  0.2719,  ...,  0.1653,  0.3191,  0.4857]],\n",
      "\n",
      "         [[-0.4724, -0.7412, -0.2681,  ..., -0.0080,  0.0072, -0.3667],\n",
      "          [ 1.0154,  0.1076, -0.5858,  ..., -0.3616, -0.4806, -0.4388],\n",
      "          [ 0.3558, -0.1052,  0.4186,  ...,  0.1252,  0.2889, -0.0345],\n",
      "          [ 0.2816,  0.5723,  0.2435,  ..., -0.1841, -0.0644,  0.7267]],\n",
      "\n",
      "         [[ 0.2085, -0.8711,  0.7487,  ..., -0.0943, -0.1879, -0.3436],\n",
      "          [-0.3292, -0.2729,  0.5168,  ...,  0.0084, -0.0992,  0.1430],\n",
      "          [-0.7029,  0.5389,  0.2108,  ...,  0.6587, -0.3255,  0.1979],\n",
      "          [-0.1156, -0.2623, -0.4988,  ..., -0.3909, -0.4289, -0.5743]],\n",
      "\n",
      "         [[ 0.3004,  0.7182,  0.6724,  ..., -0.4278,  0.6043,  0.0655],\n",
      "          [-0.1277, -0.1441,  0.1573,  ..., -0.5108,  0.3556,  0.0182],\n",
      "          [ 0.3453,  0.3483,  0.2061,  ..., -0.0981, -0.3341,  0.2272],\n",
      "          [-0.2085, -0.0174,  0.3971,  ..., -0.3147, -0.2160, -0.0382]]]],\n",
      "       grad_fn=<PermuteBackward>)\n",
      "torch.Size([1, 128, 4, 4])\n",
      "Activated\n",
      "tensor([[[0.0000, 0.2282],\n",
      "         [0.0000, 2.3898]],\n",
      "\n",
      "        [[0.2123, 0.8153],\n",
      "         [0.0000, 0.1578]]], grad_fn=<SliceBackward>)\n",
      "Upsampled\n",
      "tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2123, 0.2123],\n",
      "         [0.2123, 0.2123]]], grad_fn=<SliceBackward>)\n",
      "AFTER conv1\n",
      "tensor([[[-0.8400,  0.5112],\n",
      "         [ 0.0105, -0.0720]],\n",
      "\n",
      "        [[ 0.3200,  0.2363],\n",
      "         [ 0.0995,  1.0059]]], grad_fn=<SliceBackward>)\n",
      "Conv weights\n",
      "tensor([[[ 0.0063,  0.0670,  0.0449],\n",
      "         [-0.0569,  0.0131, -0.0612],\n",
      "         [-0.0061, -0.0575,  0.0585]],\n",
      "\n",
      "        [[-0.0357, -0.0491,  0.0171],\n",
      "         [-0.0274, -0.0709, -0.0309],\n",
      "         [-0.0720,  0.0672,  0.0107]],\n",
      "\n",
      "        [[-0.0586, -0.0508,  0.0153],\n",
      "         [-0.0486,  0.0340, -0.0051],\n",
      "         [-0.0204,  0.0007,  0.0169]]])\n",
      "------\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "hi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4c8def3d4f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_predict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# output2 = model.forward_predict_discriminator(output1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-29545cfac0f2>\u001b[0m in \u001b[0;36mforward_predict_generator\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_predict_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1edc8f4965b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;31m#             print(\"B\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#             print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b78e0419e394>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: hi"
     ]
    }
   ],
   "source": [
    "model = GAN(gamma=gamma, noise_size=noise_size, batch_size=batch_size, num_channels=num_channels)\n",
    "model.train()\n",
    "y = torch.ones(1,128)\n",
    "output1 = model.forward_predict_generator(y)\n",
    "# output2 = model.forward_predict_discriminator(output1)\n",
    "\n",
    "print(output1.size())\n",
    "print(output1.permute(0,2,3,1)[0])\n",
    "\n",
    "# print(output2)\n",
    "\n",
    "# if use_cuda:\n",
    "#     model = model.cuda()\n",
    "#     model.discriminator = nn.DataParallel(model.discriminator)\n",
    "#     model.generator = nn.DataParallel(model.generator)\n",
    "\n",
    "# discriminator_optimizer = optim.Adam(model.discriminator.parameters(), betas=(0, 0.9), lr=base_lr)\n",
    "# generator_optimizer = optim.Adam(model.generator.parameters(), betas=(0, 0.9), lr=base_lr)\n",
    "# discriminator_lr_scheduler = optim.lr_scheduler.LambdaLR(discriminator_optimizer, lambda step: max(0, (1 - step/num_updates)))\n",
    "# generator_lr_scheduler = optim.lr_scheduler.LambdaLR(generator_optimizer, lambda step: max(0, (1 - step/num_updates)))\n",
    "# optimizers = discriminator_optimizer, generator_optimizer\n",
    "# lr_schedulers = discriminator_lr_scheduler, generator_lr_scheduler\n",
    "\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# model.train()\n",
    "# model = gan_train(model, train_dataloader, optimizers, lr_schedulers, num_updates=num_updates, \n",
    "#                   use_cuda=use_cuda, num_discriminator=num_discriminator)\n",
    "# print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_vec = py_conv_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "py_c1 = model.generator.resblocks[0].conv1.weight\n",
    "tf_c1 = np.load(\"conv1.npy\")\n",
    "print(py_c1.shape)\n",
    "print(tf_c1.shape)\n",
    "py_reversed = py_c1.permute(3,2,1,0)\n",
    "print(py_reversed[0, :3, :3, :3])\n",
    "print(\"-\" * 20)\n",
    "print(tf_c1[0, :3, :3, :3])\n",
    "print(\"==\" * 20)\n",
    "orig = np.load(\"params/generator_conv2d_kernel:0.npy\")\n",
    "print(np.around(orig[0, :3, :3, :3], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_conv_vec = np.load(\"conv_vec.npy\")\n",
    "py_conv_vec = conv_vec\n",
    "py_conv_vec = py_conv_vec.permute(0, 2, 3, 1)\n",
    "\n",
    "# tf_up_vec = np.load(\"up_vec.npy\")\n",
    "# py_up_vec = upsampled\n",
    "# py_up_vec = py_up_vec.permute(0, 2, 3, 1)\n",
    "\n",
    "print(tf_conv_vec[0, :4, :4, :4])\n",
    "print(\"=\"*10)\n",
    "print(py_conv_vec[0, :4, :4, :4])\n",
    "print(tf_conv_vec.shape)\n",
    "\n",
    "\n",
    "# print(py_act_vec[0, :4, :4, :4])\n",
    "# print(\"=\"*10)\n",
    "# print(py_up_vec[0, :4, :4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# py_up_vec = upsampled\n",
    "in_channels = 1\n",
    "kernel_size = 3\n",
    "out_channels = 2\n",
    "py_up_vec = torch.ones((1, in_channels, 4, 4))\n",
    "tf_up_vec = py_up_vec.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "print(py_up_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5335, -0.7245,  0.7922],\n",
      "          [-0.6585, -0.2911, -0.0033],\n",
      "          [ 0.0554,  0.5888, -0.7971]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1115, -0.2301,  0.1445],\n",
      "          [ 0.3641,  0.5915, -0.6733],\n",
      "          [ 0.8153,  0.2231, -0.0334]]]])\n",
      "============================================================\n",
      "torch.Size([2, 1, 3, 3])\n",
      "tensor([[[[ 1.0098,  0.5736],\n",
      "          [ 0.1211,  0.8521],\n",
      "          [ 0.1211,  0.8521],\n",
      "          [ 0.2676,  0.2967]],\n",
      "\n",
      "         [[ 0.3968, -0.1031],\n",
      "          [-1.0253,  0.7669],\n",
      "          [-1.0253,  0.7669],\n",
      "          [-0.1543,  0.8849]],\n",
      "\n",
      "         [[ 0.3968, -0.1031],\n",
      "          [-1.0253,  0.7669],\n",
      "          [-1.0253,  0.7669],\n",
      "          [-0.1543,  0.8849]],\n",
      "\n",
      "         [[ 0.3238,  0.7274],\n",
      "          [-0.4398,  1.3743],\n",
      "          [-0.4398,  1.3743],\n",
      "          [ 0.1401,  1.4589]]]], grad_fn=<PermuteBackward>)\n",
      "torch.Size([1, 4, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "same_padding = (kernel_size-1)//2\n",
    "conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=same_padding)\n",
    "\n",
    "# conv.weight.data = torch.tensor(np.load(\"params/generator_conv2d_kernel:0.npy\")).permute(2,3,0,1)#.permute(3,2,1,0)\n",
    "# conv.bias.data = torch.tensor(np.load(\"params/generator_conv2d_bias:0.npy\"))\n",
    "conv.weight.data = torch.tensor(tf_weight).permute(3,2,0,1)\n",
    "conv.bias.data = torch.tensor(tf_bias)\n",
    "# print(\"FIRST\", torch.tensor(tf_weight).permute(3,2,0,1)[0, :2, :2, :2])\n",
    "# print(\"SECOND\", torch.tensor(tf_weight).permute(3,2,1,0)[0, :2, :2, :2])\n",
    "print(conv.weight.data)\n",
    "print(\"==\" * 30)\n",
    "\n",
    "print(conv.weight.data.shape)\n",
    "conv.bias.data = torch.tensor(tf_bias)\n",
    "\n",
    "\n",
    "output = conv(py_up_vec)\n",
    "output = output.permute(0, 2, 3, 1)\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n",
    "\n",
    "# print(\"====\" * 30)\n",
    "\n",
    "# conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=same_padding)\n",
    "\n",
    "# # conv.weight.data = torch.tensor(np.load(\"params/generator_conv2d_kernel:0.npy\")).permute(2,3,0,1)#.permute(3,2,1,0)\n",
    "# # conv.bias.data = torch.tensor(np.load(\"params/generator_conv2d_bias:0.npy\"))\n",
    "# conv.weight.data = torch.tensor(tf_weight).permute(3,2,1,0)\n",
    "# conv.bias.data = torch.tensor(tf_bias)\n",
    "# print(conv.weight.data)\n",
    "# output2 = conv(py_up_vec)\n",
    "# output2 = output2.permute(0, 2, 3, 1)\n",
    "# print(output2)\n",
    "# print(output2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[ 0.7922, -0.2301,  0.0000,  0.0000],\n",
      "          [ 0.1115, -0.5335,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.8153,  0.0554,  0.0000,  0.0000],\n",
      "          [-0.0033,  0.5915,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "py_up_vec2 = torch.zeros((1, in_channels, 4, 4))\n",
    "py_up_vec2[0,0,0,0] = 1\n",
    "\n",
    "output2 = conv(py_up_vec2)\n",
    "# output2 = output2.permute(0, 2, 3, 1)\n",
    "print(output2.shape)\n",
    "# print(output2[0, 0, :, :])\n",
    "# print(output2[0, 1, :, :])\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 3])\n",
      "==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5335, -0.7245,  0.7922],\n",
       "          [-0.6585, -0.2911, -0.0033],\n",
       "          [ 0.0554,  0.5888, -0.7971]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1115, -0.2301,  0.1445],\n",
       "          [ 0.3641,  0.5915, -0.6733],\n",
       "          [ 0.8153,  0.2231, -0.0334]]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(conv.weight.data.shape)\n",
    "print(\"==\")\n",
    "conv.weight.data[:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 1\n",
      "1 2 3 1\n",
      "INPUT---------\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "OUTPUT\n",
      "tensor([[[[-0.5335,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5915,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "WAT?\n",
      "tensor([[[[ 0.7922, -0.2301,  0.0000,  0.0000],\n",
      "          [ 0.1115, -0.5335,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.8153,  0.0554,  0.0000,  0.0000],\n",
      "          [-0.0033,  0.5915,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "------------------------------\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(in_channels, out_channels, kernel_size, same_padding)\n",
    "print(1,2,3,1)\n",
    "conv_double = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=0)\n",
    "conv_double.weight.data = torch.tensor(tf_weight).permute(3,2,0,1)\n",
    "conv_double.bias.data = torch.tensor(tf_bias)\n",
    "\n",
    "print(\"INPUT---------\")\n",
    "py_up_vec3 = torch.zeros((1, in_channels, 6, 6))\n",
    "py_up_vec3[0,0,0,0] = 1\n",
    "print(py_up_vec3[0,0])\n",
    "\n",
    "\n",
    "output3 = conv_double(py_up_vec3)\n",
    "print(\"OUTPUT\")\n",
    "print(output3)\n",
    "\n",
    "print(\"WAT?\")\n",
    "print(output2)\n",
    "# print(output3 / 2)\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(np.array_equal(conv.weight.data, conv_double.weight.data))\n",
    "print(np.array_equal(conv.bias.data, conv_double.bias.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 3])\n",
      "tensor([[-0.5335, -0.7245,  0.7922],\n",
      "        [-0.6585, -0.2911, -0.0033],\n",
      "        [ 0.0554,  0.5888, -0.7971]])\n",
      "tensor(-1.5716)\n",
      "tensor(1.3133)\n"
     ]
    }
   ],
   "source": [
    "print(conv.weight.data.shape)\n",
    "first_weights = conv.weight.data[0,0]\n",
    "print(first_weights)\n",
    "print(torch.sum(first_weights))\n",
    "print(torch.sum(conv.weight.data[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-52a9c6741206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "print(conv.weight.data.shape)\n",
    "print(conv.weight.data[0, 7, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.5334671  -0.7244778   0.7921703 ]\n",
      "   [-0.65852064 -0.29105836 -0.00331324]\n",
      "   [ 0.05537641  0.58875823 -0.7970913 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.11154282 -0.23014873  0.14453804]\n",
      "   [ 0.3641212   0.5914984  -0.67334676]\n",
      "   [ 0.81529963  0.22313368 -0.03336781]]]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-8d7b8b2ea851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "arr = conv.weight.data.cpu().detach().numpy()\n",
    "print(arr)\n",
    "print(arr[0,2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5027047   0.10791749]\n",
      "  [-1.1058489   1.2873383 ]]\n",
      "\n",
      " [[-0.43501222  0.0223068 ]\n",
      "  [-1.5716236   1.3132703 ]]]\n",
      "(1, 4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "tf_arr = tf.convert_to_tensor(tf_up_vec, dtype=tf.float32)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer(uniform=True)\n",
    "result = tf.layers.conv2d(tf_arr, filters=out_channels, kernel_size=kernel_size,\n",
    "                            padding='SAME', kernel_initializer=initializer)\n",
    "sess.run([tf.global_variables_initializer(),\n",
    "          tf.local_variables_initializer()])\n",
    "np_result = result.eval()\n",
    "# print([v.name for v in tf.trainable_variables() ])\n",
    "vs = tf.trainable_variables()\n",
    "tf_weight = sess.run(vs[-2])\n",
    "tf_bias = sess.run(vs[-1])\n",
    "print(np_result[0, :2, :2, :2])\n",
    "print(np_result.shape)\n",
    "sess.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5027047  -1.1058489  -1.1058489  -0.30544436]\n",
      " [-0.43501222 -1.5716236  -1.5716236  -1.5633893 ]\n",
      " [-0.43501222 -1.5716236  -1.5716236  -1.5633893 ]\n",
      " [-0.22667915 -1.418667   -1.418667   -2.2075238 ]]\n",
      "=========\n",
      "(3, 3, 1, 2)\n",
      "[[[-0.5334671 ]\n",
      "  [-0.7244778 ]\n",
      "  [ 0.7921703 ]]\n",
      "\n",
      " [[-0.65852064]\n",
      "  [-0.29105836]\n",
      "  [-0.00331324]]\n",
      "\n",
      " [[ 0.05537641]\n",
      "  [ 0.58875823]\n",
      "  [-0.7970913 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(np_result[0,:,:,0])\n",
    "print(\"=========\")\n",
    "print(tf_weight.shape)\n",
    "print(tf_weight[:,:,:,0])\n",
    "\n",
    "# # CONCLUSION: EIHTER3210 OR 2310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.5027047"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_conv = tf_weight[:,:,:,0]\n",
    "print(last_conv.shape)\n",
    "np.sum(last_conv[-2:, -2:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5335, -0.6585,  0.0554],\n",
      "          [-0.7245, -0.2911,  0.5888],\n",
      "          [ 0.7922, -0.0033, -0.7971]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1115,  0.3641,  0.8153],\n",
      "          [-0.2301,  0.5915,  0.2231],\n",
      "          [ 0.1445, -0.6733, -0.0334]]]])\n"
     ]
    }
   ],
   "source": [
    "print(conv.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.1492607  -0.80596215]\n",
      "   [ 0.6515763   0.31801212]\n",
      "   [ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[-0.11954015  0.41798925]\n",
      "   [ 0.02802968 -0.3155235 ]\n",
      "   [ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]\n",
      "   [ 0.          0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.        ]\n",
      "   [ 0.          0.        ]\n",
      "   [ 0.          0.        ]\n",
      "   [ 0.          0.        ]]]]\n",
      "(1, 4, 4, 2)\n",
      "============\n",
      "[[[[ 0.02802968 -0.3155235 ]]\n",
      "\n",
      "  [[-0.11954015  0.41798925]]\n",
      "\n",
      "  [[-0.78344625 -0.05473381]]]\n",
      "\n",
      "\n",
      " [[[ 0.6515763   0.31801212]]\n",
      "\n",
      "  [[-0.1492607  -0.80596215]]\n",
      "\n",
      "  [[-0.3441969  -0.47510624]]]\n",
      "\n",
      "\n",
      " [[[ 0.4044745  -0.5900563 ]]\n",
      "\n",
      "  [[-0.4387843   0.6062478 ]]\n",
      "\n",
      "  [[ 0.45067    -0.1505813 ]]]]\n",
      "(3, 3, 1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi2277/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "tf_arr = tf.convert_to_tensor(py_up_vec2.permute(0,2,3,1).cpu().detach().numpy(), dtype=tf.float32)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer(uniform=True)\n",
    "result = tf.layers.conv2d(tf_arr, filters=out_channels, kernel_size=kernel_size,\n",
    "                            padding='SAME', kernel_initializer=initializer)\n",
    "sess.run([tf.global_variables_initializer(),\n",
    "          tf.local_variables_initializer()])\n",
    "np_result2 = result.eval()\n",
    "# print([v.name for v in tf.trainable_variables() ])\n",
    "vs = tf.trainable_variables()\n",
    "tf_weight2 = sess.run(vs[-2])\n",
    "# tf_bias = sess.run(vs[-1])\n",
    "print(np_result2)\n",
    "print(np_result2.shape)\n",
    "print(\"============\")\n",
    "print(tf_weight2)\n",
    "print(tf_weight2.shape)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1492607   0.6515763   0.          0.        ]\n",
      " [-0.11954015  0.02802968  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "---------------\n",
      "[[ 0.02802968 -0.11954015 -0.78344625]\n",
      " [ 0.6515763  -0.1492607  -0.3441969 ]\n",
      " [ 0.4044745  -0.4387843   0.45067   ]]\n"
     ]
    }
   ],
   "source": [
    "print(np_result2[0,:,:,0])\n",
    "print(\"---------------\")\n",
    "print(tf_weight2[:,:,0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-49fba65cff19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_trans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "test_indexing = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(test_indexing[7,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Data\n",
      "tensor([[[[ 2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.],\n",
      "          [ 8.,  9., 10.]]],\n",
      "\n",
      "\n",
      "        [[[11., 12., 13.],\n",
      "          [14., 15., 16.],\n",
      "          [17., 18., 19.]]]])\n",
      "##############################\n",
      "output\n",
      "torch.Size([1, 2, 2, 2])\n",
      "tensor([[[[ 2.,  0.],\n",
      "          [ 0.,  0.]],\n",
      "\n",
      "         [[11.,  0.],\n",
      "          [ 0.,  0.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "==============================\n",
      "torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[ 6.,  5.,  0.,  0.],\n",
      "          [ 3.,  2.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.]],\n",
      "\n",
      "         [[15., 14.,  0.,  0.],\n",
      "          [12., 11.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "in_channels = 1\n",
    "kernel_size = 3\n",
    "out_channels = 2\n",
    "padding = 0\n",
    "img_size = 4\n",
    "\n",
    "conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "conv.weight.data = torch.FloatTensor(range(2,20)).view(out_channels, in_channels, kernel_size, kernel_size)\n",
    "print(\"Weight Data\")\n",
    "print(conv.weight.data)\n",
    "conv.bias.data = torch.zeros(out_channels)\n",
    "print(\"#\" * 30)\n",
    "\n",
    "input_tensor = torch.zeros((1, in_channels, img_size, img_size))\n",
    "input_tensor[0,0,0,0] = 1\n",
    "\n",
    "print('output')\n",
    "output = conv(input_tensor)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "print(\"===\" * 10)\n",
    "padding = 1\n",
    "conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "conv2.weight.data = torch.FloatTensor(range(2,20)).view(out_channels, in_channels, kernel_size, kernel_size)\n",
    "conv2.bias.data = torch.zeros(out_channels)\n",
    "output2 = conv2(input_tensor)\n",
    "print(output2.shape)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT------------\n",
      "tensor([[[[-0.2911, -0.6585,  0.0000,  0.0000],\n",
      "          [-0.7245, -0.5335,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5915,  0.3641,  0.0000,  0.0000],\n",
      "          [-0.2301,  0.1115,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnConvolutionBackward>)\n",
      "============================================================\n",
      "tensor([[[[-0.2911, -0.6585,  0.0000,  0.0000],\n",
      "          [-0.7245, -0.5335,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5915,  0.3641,  0.0000,  0.0000],\n",
      "          [-0.2301,  0.1115,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnConvolutionBackward>)\n",
      "======= verifications ===========\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "in_channels = 1\n",
    "kernel_size = 3\n",
    "out_channels = 2\n",
    "padding = 1\n",
    "img_size = 4\n",
    "\n",
    "\n",
    "conv_real = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "conv_real.weight.data = torch.tensor(np.load(\"conv_weight.npy\"))\n",
    "conv_real.bias.data = torch.tensor(np.load(\"conv_bias.npy\"))\n",
    "conv_real = conv_real.cuda()\n",
    "\n",
    "# print(\"INPUT---------\")\n",
    "py_up_vec3 = torch.zeros((1, in_channels, 4, 4))\n",
    "py_up_vec3[0,0,0,0] = 1\n",
    "py_up_vec3 = py_up_vec3.cuda()\n",
    "# print(py_up_vec3[0,0])\n",
    "\n",
    "\n",
    "output3 = conv_real(py_up_vec3)\n",
    "print(\"OUTPUT------------\")\n",
    "print(output3)\n",
    "\n",
    "print(\"==\" * 30)\n",
    "\n",
    "conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "conv.weight.data = torch.tensor(np.load(\"tf_weight.npy\")).permute(3,2,0,1) #BAD\n",
    "# conv.weight.data = torch.tensor(tf_weight).permute(3,2,0,1) # BAD\n",
    "# conv.weight.data = torch.tensor(np.load(\"conv_weight.npy\")) #GOOD\n",
    "# conv.weight.data = torch.tensor(np.load(\"tf_weight.npy\")).permute(3,2,0,1).clone() #GOOD\n",
    "# conv.weight.data = torch.tensor(np.load(\"tf_weight.npy\")).permute(3,2,0,1).contiguous()\n",
    "conv.bias.data = torch.tensor(tf_bias)\n",
    "conv = conv.cuda()\n",
    "\n",
    "\n",
    "output2 = conv(py_up_vec3)\n",
    "# print(output2.shape)\n",
    "print(output2)\n",
    "\n",
    "print(\"======= verifications ===========\")\n",
    "\n",
    "print(np.array_equal(conv.weight.data, conv_real.weight.data))\n",
    "print(np.array_equal(conv.bias.data, conv_real.bias.data))\n",
    "print(conv.in_channels == conv_real.in_channels)\n",
    "print(conv.out_channels == conv_real.out_channels)\n",
    "print(conv.padding == conv_real.padding)\n",
    "print(conv.kernel_size == conv_real.kernel_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"WEIGHT=================\")\n",
    "# print(conv_real.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight.data = torch.tensor(tf_weight).permute(3,2,0,1)\n",
    "np.save(\"tf_weight.npy\", tf_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[-0.2911, -0.6585,  0.0000,  0.0000],\n",
      "          [-0.7245, -0.5335,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.5915,  0.3641,  0.0000,  0.0000],\n",
      "          [-0.2301,  0.1115,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "============================================================\n",
      "torch.Size([2, 1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5335, -0.7245,  0.7922],\n",
       "          [-0.6585, -0.2911, -0.0033],\n",
       "          [ 0.0554,  0.5888, -0.7971]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1115, -0.2301,  0.1445],\n",
       "          [ 0.3641,  0.5915, -0.6733],\n",
       "          [ 0.8153,  0.2231, -0.0334]]]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_up_vec2 = torch.zeros((1, in_channels, 4, 4))\n",
    "py_up_vec2[0,0,0,0] = 1\n",
    "\n",
    "\n",
    "print(conv.weight.data.shape)\n",
    "conv.weight.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
