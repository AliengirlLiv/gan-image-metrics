{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Stdout/stderr redirector, at the OS level, using file descriptors.\n",
    "\n",
    "This also works under windows.\n",
    "\"\"\"\n",
    "\n",
    "__docformat__ = \"restructuredtext en\"\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "#  Copyright (C) 2008  The IPython Development Team\n",
    "#\n",
    "#  Distributed under the terms of the BSD License.  The full license is in\n",
    "#  the file COPYING, distributed as part of this software.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "STDOUT = 1\n",
    "STDERR = 2\n",
    "\n",
    "class FDRedirector(object):\n",
    "    \"\"\" Class to redirect output (stdout or stderr) at the OS level using\n",
    "        file descriptors.\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, fd=STDOUT):\n",
    "        \"\"\" fd is the file descriptor of the outpout you want to capture.\n",
    "            It can be STDOUT or STERR.\n",
    "        \"\"\"\n",
    "        self.fd = fd\n",
    "        self.started = False\n",
    "        self.piper = None\n",
    "        self.pipew = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\" Setup the redirection.\n",
    "        \"\"\"\n",
    "        if not self.started:\n",
    "            self.oldhandle = os.dup(self.fd)\n",
    "            self.piper, self.pipew = os.pipe()\n",
    "            os.dup2(self.pipew, self.fd)\n",
    "            os.close(self.pipew)\n",
    "\n",
    "            self.started = True\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\" Flush the captured output, similar to the flush method of any\n",
    "        stream.\n",
    "        \"\"\"\n",
    "        if self.fd == STDOUT:\n",
    "            sys.stdout.flush()\n",
    "        elif self.fd == STDERR:\n",
    "            sys.stderr.flush()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\" Unset the redirection and return the captured output. \n",
    "        \"\"\"\n",
    "        if self.started:\n",
    "            self.flush()\n",
    "            os.dup2(self.oldhandle, self.fd)\n",
    "            os.close(self.oldhandle)\n",
    "            f = os.fdopen(self.piper, 'r')\n",
    "            output = f.read()\n",
    "            f.close()\n",
    "\n",
    "            self.started = False\n",
    "            return output\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def getvalue(self):\n",
    "        \"\"\" Return the output captured since the last getvalue, or the\n",
    "        start of the redirection.\n",
    "        \"\"\"\n",
    "        output = self.stop()\n",
    "        self.start()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM https://github.com/adler-j/adler\n",
    "\n",
    "import demandimport\n",
    "with demandimport.enabled():\n",
    "    import tensorflow as tf\n",
    "\n",
    "    \n",
    "__all__ = ('image_grid', 'image_grid_summary', 'scalars_summary')\n",
    "\n",
    "\n",
    "def image_grid(x, size=8):\n",
    "    t = tf.unstack(x[:size * size], num=size*size, axis=0)\n",
    "    rows = [tf.concat(t[i*size:(i+1)*size], axis=0) for i in range(size)]\n",
    "    image = tf.concat(rows, axis=1)\n",
    "    return image[None]\n",
    "\n",
    "\n",
    "def image_grid_summary(name, x):\n",
    "    with tf.name_scope(name):\n",
    "        tf.summary.image('grid', image_grid(x))\n",
    "\n",
    "\n",
    "def scalars_summary(name, x):\n",
    "    with tf.name_scope(name):\n",
    "        x = tf.reshape(x, [-1])\n",
    "        mean, var = tf.nn.moments(x, axes=0)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        tf.summary.scalar('std', tf.sqrt(var))\n",
    "        tf.summary.histogram('histogram', x)\n",
    "        \n",
    "import os\n",
    "import shutil\n",
    "from os.path import join, expanduser, exists\n",
    "\n",
    "import demandimport\n",
    "with demandimport.enabled():\n",
    "    import tensorflow as tf\n",
    "\n",
    "__all__ = ('get_base_dir',\n",
    "           'default_checkpoint_path', 'default_tensorboard_dir',\n",
    "           'summary_writers')\n",
    "\n",
    "\n",
    "def get_base_dir():\n",
    "    \"\"\"Get the data directory.\"\"\"\n",
    "    base_odl_dir = os.environ.get('ADLER_HOME',\n",
    "                                  expanduser(join('~', '.adler')))\n",
    "    data_home = join(base_odl_dir, 'tensorflow')\n",
    "    if not exists(data_home):\n",
    "        os.makedirs(data_home)\n",
    "    return data_home\n",
    "\n",
    "\n",
    "def default_checkpoint_path(name):\n",
    "    checkpoint_dir = join(get_base_dir(), 'checkpoints')\n",
    "    if not exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    checkpoint_path = join(checkpoint_dir,\n",
    "                           '{}.ckpt'.format(name))\n",
    "\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def default_tensorboard_dir(name):\n",
    "    tensorboard_dir = join(get_base_dir(), 'tensorboard', name)\n",
    "    if not exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "    return tensorboard_dir\n",
    "\n",
    "\n",
    "def summary_writers(name, cleanup=False, session=None, write_graph=True):\n",
    "    if session is None:\n",
    "        session = tf.get_default_session()\n",
    "\n",
    "    dname = default_tensorboard_dir(name)\n",
    "\n",
    "    if cleanup and os.path.exists(dname):\n",
    "        shutil.rmtree(dname, ignore_errors=True)\n",
    "\n",
    "    if write_graph:\n",
    "        graph = session.graph\n",
    "    else:\n",
    "        graph = None\n",
    "\n",
    "    test_summary_writer = tf.summary.FileWriter(dname + '/test', graph)\n",
    "    train_summary_writer = tf.summary.FileWriter(dname + '/train')\n",
    "\n",
    "    return test_summary_writer, train_summary_writer\n",
    "\n",
    "\n",
    "import demandimport\n",
    "with demandimport.enabled():\n",
    "    import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cosine_decay(learning_rate, global_step, maximum_steps,\n",
    "                 name=None):\n",
    "    from tensorflow.python.ops import math_ops\n",
    "    from tensorflow.python.framework import ops\n",
    "\n",
    "    if global_step is None:\n",
    "        raise ValueError(\"global_step is required for cosine_decay.\")\n",
    "    with ops.name_scope(name, \"CosineDecay\",\n",
    "                      [learning_rate, global_step, maximum_steps]) as name:\n",
    "        learning_rate = ops.convert_to_tensor(learning_rate, name=\"learning_rate\")\n",
    "        dtype = learning_rate.dtype\n",
    "        global_step = math_ops.cast(global_step, dtype)\n",
    "        maximum_steps = math_ops.cast(maximum_steps, dtype)\n",
    "\n",
    "        p = tf.mod(global_step / maximum_steps, 1)\n",
    "\n",
    "    return learning_rate * (0.5 + 0.5 * math_ops.cos(p * np.pi))\n",
    "\n",
    "\n",
    "class EMAHelper(object):\n",
    "    def __init__(self, decay=0.99, session=None):\n",
    "        if session is None:\n",
    "            self.session = tf.get_default_session()\n",
    "        else:\n",
    "            self.session = session\n",
    "\n",
    "        self.all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "        self.apply = self.ema.apply(self.all_vars)\n",
    "        self.averages = [self.ema.average(var) for var in self.all_vars]\n",
    "\n",
    "    def average_dict(self):\n",
    "        ema_averages_results = self.session.run(self.averages)\n",
    "        return {var: value for var, value in zip(self.all_vars,\n",
    "                                                 ema_averages_results)}\n",
    "\n",
    "    def variables_to_restore(self):\n",
    "        return self.ema.variables_to_restore(tf.moving_average_variables())\n",
    "\n",
    "\n",
    "def ema_wrapper(is_training, decay=0.99, scope='ema_wrapper', reuse=False):\n",
    "    \"\"\"Use Exponential Moving Average of weights during testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_training : bool or `tf.Tensor` of type bool\n",
    "        Indicates if the EMA should be applied or not\n",
    "    decay:\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    During training, the current value of a is used. During testing, the\n",
    "    exponential moving average is applied instead.\n",
    "\n",
    "    >>> @ema_wrapper(is_training)\n",
    "    ... def function(x):\n",
    "    ....    a = tf.get_variable('a', [], tf.float32)\n",
    "    ...     return a * x\n",
    "    \"\"\"\n",
    "    def function(fun):\n",
    "        def fun_wrapper(*args, **kwargs):\n",
    "            with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "                # Regular call\n",
    "                with tf.variable_scope('function_call') as sc:\n",
    "                    result_train = fun(*args, **kwargs)\n",
    "\n",
    "                # Set up exponential moving average\n",
    "                ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "                var_class = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                              sc.name)\n",
    "                ema_op = ema.apply(var_class)\n",
    "\n",
    "                # Add to collection so they are updated\n",
    "                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ema_op)\n",
    "\n",
    "                # Getter for the variables with EMA applied\n",
    "                def ema_getter(getter, name, *args, **kwargs):\n",
    "                    var = getter(name, *args, **kwargs)\n",
    "                    ema_var = ema.average(var)\n",
    "                    return ema_var if ema_var else var\n",
    "\n",
    "                # Call with EMA applied\n",
    "                with tf.variable_scope('function_call',\n",
    "                                       reuse=True,\n",
    "                                       custom_getter=ema_getter):\n",
    "                    result_test = fun(*args, **kwargs)\n",
    "\n",
    "                # Return the correct version depending on if we're training or\n",
    "                # not\n",
    "                return tf.cond(is_training,\n",
    "                               lambda: result_train, lambda: result_test)\n",
    "        return fun_wrapper\n",
    "    return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utilities for computing the sobolev norm.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def sobolev_filter(x, c=5, s=1):\n",
    "    \"\"\"Apply sobolev filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensorflow.Tensor of shape B W H C\n",
    "        txt\n",
    "    c : float\n",
    "        Scaling of the cooridinate systems (1 / pixel size)\n",
    "    s : float\n",
    "        Order of the Sobolev norm\n",
    "    \"\"\"\n",
    "    with tf.name_scope('sobolev'):\n",
    "        # FFT is taken over the innermost axes, so move channel to beginning.\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        fft_x = tf.spectral.fft2d(tf.cast(x, 'complex64'))\n",
    "\n",
    "        shape = tf.shape(fft_x)\n",
    "        sx = shape[3]\n",
    "        sy = shape[2]\n",
    "\n",
    "        # Construct meshgrid for the scale\n",
    "        x = tf.range(sx)\n",
    "        x = tf.minimum(x, sx - x)\n",
    "        x = tf.cast(x, dtype='complex64') / tf.cast(sx // 2, dtype='complex64')\n",
    "        y = tf.range(sy)\n",
    "        y = tf.minimum(y, sy - y)\n",
    "        y = tf.cast(y, dtype='complex64') / tf.cast(sy // 2, dtype='complex64')\n",
    "        X, Y = tf.meshgrid(x, y)\n",
    "        X = X[None, None]\n",
    "        Y = Y[None, None]\n",
    "\n",
    "        scale = (1 + c * (X ** 2 + Y ** 2)) ** (s / 2)\n",
    "\n",
    "        # Compute spatial gradient in fourier space\n",
    "        fft_x = scale * fft_x\n",
    "\n",
    "        result_x = tf.spectral.ifft2d(fft_x)\n",
    "        result_x = tf.real(result_x)\n",
    "        return tf.transpose(result_x, [0, 2, 3, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "def print_param(v):\n",
    "    print(\"Name: \", v.name)\n",
    "    vector = sess.run(v)\n",
    "    print(\"Shape: \", v.shape)\n",
    "    print(vector)\n",
    "    print(\"=\" * 20)\n",
    "    name = v.name.replace(\"/\", \"_\")\n",
    "    np.save(\"params/\" + name, vector)\n",
    "    \n",
    "def print_all():\n",
    "    [print_param(v) for v in tf.trainable_variables() ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Source: https://gist.github.com/vihari/f9b361058825e16d390f0e443bfdffc7\n",
    "# global vec0\n",
    "vec0 = []\n",
    "vec1 = []\n",
    "vec2 = []\n",
    "\n",
    "def tf_print(index, op, tensors, message=None):\n",
    "\n",
    "\n",
    "    def print_message(x):\n",
    "        if index == 0:\n",
    "            global vec0\n",
    "            vec0.append(x)\n",
    "        elif index == 1:\n",
    "            global vec1\n",
    "            vec1.append(x)\n",
    "        elif index == 2:\n",
    "            global vec2\n",
    "            vec2.append(x)\n",
    "\n",
    "        sys.stdout.write(message + \" %s\\n\" % x[0,:2,:2,:2])\n",
    "        return x\n",
    "\n",
    "    prints = [tf.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "    with tf.control_dependencies(prints):\n",
    "        op = tf.identity(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/adler-j/bwgan\n",
    "\"\"\"Code for training Banach Wasserstein GAN on CIFAR 10.\n",
    "\n",
    "With all the dependencies installed, the code should run as-is. \n",
    "Data is downloaded on the fly.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensordata\n",
    "import functools\n",
    "\n",
    "# User selectable parameters\n",
    "EXPONENT = 2\n",
    "SOBOLEV_C = 5.0\n",
    "SOBOLEV_S = 0\n",
    "MAX_ITERS = 100000\n",
    "SUMMARY_FREQ = 10\n",
    "INCEPTION_FREQ = 1000\n",
    "BATCH_SIZE = 64\n",
    "BATCH_SIZE_TEST = 100\n",
    "reset = True\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Training specific parameters\n",
    "name = 'cifar10_sobolev_5/p={}s={}'.format(EXPONENT, SOBOLEV_S)\n",
    "size = 32\n",
    "DUAL_EXPONENT = 1 / (1 - 1/EXPONENT) if EXPONENT != 1 else np.inf\n",
    "\n",
    "\n",
    "with tf.name_scope('placeholders'):\n",
    "#     x_train_ph, _ = tensordata.get_cifar10_tf(batch_size=BATCH_SIZE)\n",
    "    x_test_ph, _ = tensordata.get_cifar10_tf(batch_size=BATCH_SIZE_TEST)\n",
    "\n",
    "    is_training = tf.placeholder(bool, name='is_training')\n",
    "    use_agumentation = tf.identity(is_training, name='is_training')\n",
    "\n",
    "\n",
    "with tf.name_scope('pre_process'):\n",
    "#     x_train = (x_train_ph - 0.5) * 2.0\n",
    "    x_test = (x_test_ph - 0.5) * 2.0\n",
    "    x_true = x_test\n",
    "\n",
    "#     x_true = tf.cond(is_training,\n",
    "#                      lambda: x_train,\n",
    "#                      lambda: x_test)\n",
    "\n",
    "def apply_conv(x, filters=32, kernel_size=3, he_init=True, print_time=False):\n",
    "    if he_init:\n",
    "        initializer = tf.contrib.layers.variance_scaling_initializer(uniform=True)\n",
    "    else:\n",
    "        initializer = tf.contrib.layers.xavier_initializer(uniform=True)\n",
    "\n",
    "    return tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size,\n",
    "                            padding='SAME', kernel_initializer=initializer)\n",
    "\n",
    "\n",
    "def activation(x):\n",
    "    with tf.name_scope('activation'):\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def bn(x, print_time=False):\n",
    "    if print_time:\n",
    "        before_bn = tf_print(0, x, [x], \"BN INPUT: I FIXED THIS ALREADY\")\n",
    "        result= tf.layers.batch_normalization(before_bn,\n",
    "                                        momentum=0.9, # OK\n",
    "                                        center=True, # OK\n",
    "                                        scale=True, #OK\n",
    "                                        epsilon=1e-5,#OK\n",
    "    #                                     zero_debias_moving_mean=False,\n",
    "                                        training=True)\n",
    "        return tf_print(1, result, [result], \"BN RESULT: THIS SHOULD BE WORKING!!!\")\n",
    "    else:\n",
    "        return tf.layers.batch_normalization(x,\n",
    "                                        momentum=0.9, # OK\n",
    "                                        center=True, # OK\n",
    "                                        scale=True, #OK\n",
    "                                        epsilon=1e-5,#OK\n",
    "    #                                     zero_debias_moving_mean=False,\n",
    "                                        training=True)\n",
    "\n",
    "\n",
    "\n",
    "def stable_norm(x, ord):\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    alpha = tf.reduce_max(tf.abs(x) + 1e-5, axis=1)\n",
    "    result = alpha * tf.norm(x / alpha[:, None], ord=ord, axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def downsample(x):\n",
    "    with tf.name_scope('downsample'):\n",
    "        x = tf.identity(x)\n",
    "        return tf.add_n([x[:,::2,::2,:], x[:,1::2,::2,:],\n",
    "                         x[:,::2,1::2,:], x[:,1::2,1::2,:]]) / 4.\n",
    "\n",
    "def upsample(x, print_time=False):\n",
    "    with tf.name_scope('upsample'):\n",
    "        if print_time:\n",
    "            x_print1 = tf_print(3, x, [x], message=\"UPSAMPLING\")\n",
    "            x = tf.identity(x_print1)\n",
    "            x = tf.concat([x, x, x, x], axis=-1)\n",
    "            x_print2 = tf_print(4, x, [x], message=\"Weird reshape\")\n",
    "\n",
    "            upsampled = tf.depth_to_space(x_print2, 2)\n",
    "            return tf_print(2, upsampled, [upsampled], message=\"Upsampled\")\n",
    "        else:\n",
    "            x = tf.identity(x)\n",
    "            x = tf.concat([x, x, x, x], axis=-1)\n",
    "            upsampled = tf.depth_to_space(x, 2)\n",
    "            return upsampled\n",
    "\n",
    "\n",
    "def conv_meanpool(x, print_time=False, **kwargs):\n",
    "    return downsample(apply_conv(x, **kwargs))\n",
    "\n",
    "def meanpool_conv(x, print_time=False, **kwargs):\n",
    "    return apply_conv(downsample(x), **kwargs)\n",
    "\n",
    "def upsample_conv(x, print_time=False, **kwargs):\n",
    "    return apply_conv(upsample(x, print_time=print_time), **kwargs)\n",
    "\n",
    "def identity(x, print_time=False):\n",
    "    return tf.identity(x)\n",
    "\n",
    "def resblock(x, filters, resample=None, normalize=False, print_time=False):\n",
    "    if normalize:\n",
    "        norm_fn = bn\n",
    "    else:\n",
    "        norm_fn = identity\n",
    "\n",
    "    if resample == 'down':\n",
    "        conv_1 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_2 = functools.partial(conv_meanpool, filters=filters)\n",
    "        conv_shortcut = functools.partial(conv_meanpool, filters=filters,\n",
    "                                          kernel_size=1, he_init=False)\n",
    "    elif resample == 'up':\n",
    "        conv_1 = functools.partial(upsample_conv, filters=filters)\n",
    "        conv_2 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_shortcut = functools.partial(upsample_conv, filters=filters,\n",
    "                                          kernel_size=1, he_init=False)\n",
    "    elif resample == None:\n",
    "        conv_1 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_2 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_shortcut = tf.identity\n",
    "\n",
    "    with tf.name_scope('resblock'):\n",
    "        x = tf.identity(x)\n",
    "        normed = norm_fn(x)\n",
    "        activated = activation(normed)\n",
    "        activation_printed = tf_print(7, activated, [activated], message=\"Activated\")\n",
    "        update = conv_1(activation_printed, print_time=True)\n",
    "        update_print = tf_print(7, update, [update], message=\"AFTER conv1\")\n",
    "        normed_again = norm_fn(update_print, print_time=print_time)\n",
    "        normed_again_print = tf_print(7, normed_again, [normed_again], message=\"Norm again\")\n",
    "        activated_again = activation(normed_again_print)\n",
    "        activated_again_print = tf_print(7, activated_again, [activated_again], message=\"activated again\")\n",
    "        update = conv_2(activated_again_print)\n",
    "        update_print2 = tf_print(7, update, [update], message=\"AFTER conv2\")\n",
    "        skip = conv_shortcut(x)\n",
    "        return skip + update_print2\n",
    "\n",
    "\n",
    "def resblock_optimized(x, filters):\n",
    "    with tf.name_scope('resblock'):\n",
    "        x = tf.identity(x)\n",
    "        update = apply_conv(x, filters=filters)\n",
    "        update = conv_meanpool(activation(update), filters=filters)\n",
    "\n",
    "        skip = meanpool_conv(x, filters=128, kernel_size=1, he_init=False)\n",
    "        return skip + update\n",
    "\n",
    "\n",
    "def generator(z, reuse):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        with tf.name_scope('pre_process'):\n",
    "            z = tf.layers.dense(z, 4 * 4 * 128)\n",
    "            x = tf.reshape(z, [-1, 4, 4, 128])\n",
    "\n",
    "        with tf.name_scope('x1'):\n",
    "            x = resblock(x, filters=128, resample='up', normalize=True, print_time=True) # 8\n",
    "            x = resblock(x, filters=128, resample='up', normalize=True) # 16\n",
    "            x = resblock(x, filters=128, resample='up', normalize=True) # 32\n",
    "\n",
    "        with tf.name_scope('post_process'):\n",
    "            x = activation(bn(x))\n",
    "            result = apply_conv(x, filters=3, he_init=False)\n",
    "            return tf.tanh(result)\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(x, reuse):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        with tf.name_scope('pre_process'):\n",
    "            x = resblock_optimized(x, filters=128)\n",
    "\n",
    "        with tf.name_scope('x1'):\n",
    "            x = resblock(x, filters=128, resample='down') # 8\n",
    "            x = resblock(x, filters=128) # 16\n",
    "            x = resblock(x, filters=128) # 32\n",
    "\n",
    "        with tf.name_scope('post_process'):\n",
    "            x = activation(x)\n",
    "            x = tf.reduce_mean(x, axis=[1, 2])\n",
    "            flat = tf.contrib.layers.flatten(x)\n",
    "            flat = tf.layers.dense(flat, 1)\n",
    "            return flat\n",
    "\n",
    "\n",
    "with tf.name_scope('gan'):\n",
    "    # z = tf.random_normal([tf.shape(x_true)[0], 128], name=\"z\")\n",
    "    z = tf.placeholder(tf.float32, name=\"z\")\n",
    "\n",
    "    x_generated = generator(z, reuse=False)\n",
    "\n",
    "    d_true = discriminator(x_true, reuse=False)\n",
    "    d_generated = discriminator(x_generated, reuse=True)\n",
    "\n",
    "    # z_gen = tf.random_normal([BATCH_SIZE * 2, 128], name=\"z\")\n",
    "    # d_generated_train = discriminator(generator(z_gen, reuse=True), reuse=True)\n",
    "\n",
    "with tf.name_scope('dual_norm'):\n",
    "    sobolev_true = sobolev_filter(x_true, c=SOBOLEV_C, s=SOBOLEV_S)\n",
    "    lamb = tf.reduce_mean(stable_norm(sobolev_true, ord=EXPONENT))\n",
    "    dual_sobolev_true = sobolev_filter(x_true, c=SOBOLEV_C, s=-SOBOLEV_S)\n",
    "    gamma = tf.reduce_mean(stable_norm(sobolev_true, ord=DUAL_EXPONENT))\n",
    "\n",
    "with tf.name_scope('regularizer'):\n",
    "    epsilon = tf.random_uniform([tf.shape(x_true)[0], 1, 1, 1], 0.0, 1.0)\n",
    "    x_hat = epsilon * x_generated + (1 - epsilon) * x_true\n",
    "    d_hat = discriminator(x_hat, reuse=True)\n",
    "\n",
    "    gradients = tf.gradients(d_hat, x_hat)[0]\n",
    "    dual_sobolev_gradients = sobolev_filter(gradients, c=SOBOLEV_C, s=-SOBOLEV_S)\n",
    "    ddx = stable_norm(dual_sobolev_gradients, ord=DUAL_EXPONENT)\n",
    "\n",
    "    d_regularizer = tf.reduce_mean(tf.square(ddx / gamma - 1))\n",
    "    d_regularizer_mean = tf.reduce_mean(tf.square(d_true))\n",
    "\n",
    "with tf.name_scope('loss_gan'):\n",
    "    wasserstein_scaled = (tf.reduce_mean(d_generated) - tf.reduce_mean(d_true))\n",
    "    wasserstein = wasserstein_scaled / gamma\n",
    "\n",
    "    g_loss = tf.reduce_mean(d_generated) / gamma\n",
    "    d_loss = (-wasserstein +\n",
    "              lamb * d_regularizer +\n",
    "              1e-5 * d_regularizer_mean)\n",
    "\n",
    "# with tf.name_scope('optimizer'):\n",
    "#     ema = EMAHelper(decay=0.99)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "#     decay = tf.maximum(0., 1.-(tf.cast(global_step, tf.float32)/MAX_ITERS))\n",
    "#     learning_rate = 2e-4 * decay\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0., beta2=0.9)\n",
    "\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='gan/generator')\n",
    "#     g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "#     with tf.control_dependencies(update_ops):\n",
    "#         g_train = optimizer.minimize(g_loss, var_list=g_vars,\n",
    "#                                      global_step=global_step)\n",
    "\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='gan/discriminator')\n",
    "#     d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "#     with tf.control_dependencies(update_ops):\n",
    "#         d_train = optimizer.minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "\n",
    "# with tf.name_scope('summaries'):\n",
    "#     tf.summary.scalar('wasserstein_scaled', wasserstein_scaled)\n",
    "#     tf.summary.scalar('wasserstein', wasserstein)\n",
    "\n",
    "#     tf.summary.scalar('g_loss', g_loss)\n",
    "\n",
    "#     tf.summary.scalar('d_loss', d_loss)\n",
    "#     scalars_summary('d_true', d_true)\n",
    "#     scalars_summary('d_generated', d_generated)\n",
    "#     tf.summary.scalar('d_regularizer', d_regularizer)\n",
    "#     tf.summary.scalar('d_regularizer_mean', d_regularizer_mean)\n",
    "\n",
    "#     tf.summary.scalar('learning_rate', learning_rate)\n",
    "#     tf.summary.scalar('global_step', global_step)\n",
    "\n",
    "#     scalars_summary('x_generated', x_generated)\n",
    "#     scalars_summary('x_true', x_true)\n",
    "\n",
    "#     scalars_summary('gamma', gamma)\n",
    "#     scalars_summary('lamb', lamb)\n",
    "\n",
    "#     image_grid_summary('x_true', x_true)\n",
    "#     image_grid_summary('x_generated', x_generated)\n",
    "#     image_grid_summary('gradients', gradients)\n",
    "#     image_grid_summary('dual_sobolev_gradients', dual_sobolev_gradients)\n",
    "\n",
    "#     scalars_summary('ddx', ddx)\n",
    "#     scalars_summary('gradients', gradients)\n",
    "#     scalars_summary('dual_sobolev_gradients', dual_sobolev_gradients)\n",
    "\n",
    "#     merged_summary = tf.summary.merge_all()\n",
    "\n",
    "#     # Advanced metrics\n",
    "#     with tf.name_scope('inception'):\n",
    "#         # Specific function to compute inception score for very large\n",
    "#         # number of samples\n",
    "#         def generate_and_classify(z):\n",
    "#             INCEPTION_OUTPUT = 'logits:0'\n",
    "#             x = generator(z, reuse=True)\n",
    "#             x = tf.image.resize_bilinear(x, [299, 299])\n",
    "#             return tf.contrib.gan.eval.run_inception(x, output_tensor=INCEPTION_OUTPUT)\n",
    "\n",
    "#         # Fixed z for fairness between runs\n",
    "#         inception_z = tf.constant(np.random.randn(10000, 128), dtype='float32')\n",
    "#         inception_score = tf.contrib.gan.eval.classifier_score(inception_z,\n",
    "#                                                                classifier_fn=generate_and_classify,\n",
    "#                                                                num_batches=10000 // 100)\n",
    "\n",
    "#         inception_summary = tf.summary.merge([\n",
    "#                 tf.summary.scalar('inception_score', inception_score)])\n",
    "\n",
    "#         full_summary = tf.summary.merge([merged_summary, inception_summary])\n",
    "\n",
    "#     test_summary_writer, train_summary_writer = summary_writers(name, cleanup=reset, write_graph=False)\n",
    "#\n",
    "# Initialize all TF variables\n",
    "sess.run([tf.global_variables_initializer(),\n",
    "          tf.local_variables_initializer()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print_all()\n",
    "\n",
    "# assert False\n",
    "\n",
    "# # Coordinate the loading of image files.\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "# # Add op to save and restore\n",
    "# saver = tf.train.Saver()\n",
    "# if not reset:\n",
    "#     saver.restore(sess, default_checkpoint_path(name))\n",
    "\n",
    "# # Standardized validation z\n",
    "# z_validate = np.random.randn(BATCH_SIZE_TEST, 128)\n",
    "\n",
    "# # Train the network\n",
    "# while True:\n",
    "#     i = sess.run(global_step)\n",
    "#     if i >= MAX_ITERS:\n",
    "#         break\n",
    "\n",
    "#     num_d_train = 5\n",
    "#     for j in range(num_d_train):\n",
    "#         _, d_loss_result = sess.run([d_train, d_loss],\n",
    "#                                     feed_dict={is_training: True})\n",
    "\n",
    "#     _, g_loss_result, _ = sess.run([g_train, g_loss, ema.apply],\n",
    "#              feed_dict={is_training: True})\n",
    "\n",
    "#     print('s={}, i={}, j={}, d_loss={}, g_loss={}'.format(SOBOLEV_S, i, j,\n",
    "#                                                     d_loss_result,\n",
    "#                                                     g_loss_result))\n",
    "\n",
    "#     if i % SUMMARY_FREQ == SUMMARY_FREQ - 1:\n",
    "#         ema_dict = ema.average_dict()\n",
    "#         merged_summary_result_train = sess.run(merged_summary,\n",
    "#                                          feed_dict={is_training: False,\n",
    "#                                                     **ema_dict})\n",
    "#         train_summary_writer.add_summary(merged_summary_result_train, i)\n",
    "#     if i % INCEPTION_FREQ == INCEPTION_FREQ - 1:\n",
    "#         ema_dict = ema.average_dict()\n",
    "#         merged_summary_result_test = sess.run(full_summary,\n",
    "#                                          feed_dict={z: z_validate,\n",
    "#                                                     is_training: False,\n",
    "#                                                     **ema_dict})\n",
    "#         test_summary_writer.add_summary(merged_summary_result_test, i)\n",
    "\n",
    "\n",
    "#     if i % 1000 == 999:\n",
    "#         saver.save(sess,default_checkpoint_path(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(input_arr):\n",
    "    STDERR = 2\n",
    "    redirect = FDRedirector(STDERR)\n",
    "    redirect.start();\n",
    "    tf_arr = tf.convert_to_tensor(input_arr, dtype=tf.float32)\n",
    "    gen_output = generator(tf_arr, reuse=True)\n",
    "    output = sess.run(gen_output, feed_dict={is_training: True})\n",
    "    print_statements = redirect.stop()\n",
    "    print(\"Print Statements\")\n",
    "    print(print_statements)\n",
    "    print(output.shape)\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(output.transpose(0,3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_discriminator(input_arr):\n",
    "    STDERR = 2\n",
    "    redirect = FDRedirector(STDERR)\n",
    "    redirect.start()\n",
    "    tf_arr = tf.convert_to_tensor(input_arr, dtype=tf.float32)\n",
    "    disc_output = discriminator(tf_arr, reuse=True)\n",
    "    output = sess.run(disc_output, feed_dict={is_training: True})\n",
    "    print_statements = redirect.stop()\n",
    "#     print(\"Print Statements\")\n",
    "#     print(print_statements)\n",
    "    print(output.shape)\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(input_arr):\n",
    "    STDERR = 2\n",
    "    redirect = FDRedirector(STDERR)\n",
    "    redirect.start()\n",
    "    tf_arr = tf.convert_to_tensor(input_arr, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.ones((1, 128))\n",
    "test_generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[print_param(v) for v in tf.trainable_variables() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"params/generator_dense_kernel:0.npy\")\n",
    "print(x.shape)\n",
    "print(x[0,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"params/generator_dense_kernel:0.npy\")\n",
    "print(np.sum(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tf_arr = np.sum(first_dense_weight, axis=0)\n",
    "# tf_arr = tf.convert_to_tensor(tf_arr, dtype=tf.float32)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf_arr = bn_input\n",
    "print(\"BN Input\", tf_arr[0,0,0,0])\n",
    "print\n",
    "bn = tf.contrib.layers.batch_norm(tf_arr, decay=0.9, # OK\n",
    "                                    center=True, # OK\n",
    "       f                              scale=True, #OK\n",
    "                                    epsilon=1e-5,#OK\n",
    "                                    zero_debias_moving_mean=False,\n",
    "                                    is_training=True)\n",
    "sess.run([tf.global_variables_initializer(),\n",
    "          tf.local_variables_initializer()])\n",
    "bn_eval = bn.eval()\n",
    "bn_comparable = np.moveaxis(bn_eval, 3, 1)\n",
    "print(\"output\")\n",
    "print(bn_comparable)\n",
    "# [print_param(v) for v in tf.trainable_variables() ]\n",
    "sess.close()\n",
    "\n",
    "#0000, 1001 --> same val = same; x__x\n",
    "#1100, 1110 --> same val = same  __x_ 1000\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"bn_input.npy\", vec0[0])\n",
    "np.save(\"bn_output.npy\", vec1[0])\n",
    "np.save(\"upsampled.npy\", vec2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vec2:\n",
    "    print(v[0,:2,:2,:2])\n",
    "    \n",
    "print(len(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN INPUT: I FIXED THIS ALREADY [[[-0.36227444  0.13549557]\n",
    "  [-0.47998536 -0.5358527 ]]\n",
    "\n",
    " [[-1.3878101   1.5702378 ]\n",
    "  [-1.9097602   0.3231582 ]]]\n",
    "BN RESULT: THIS SHOULD BE WORKING!!! [[[ 0.28651154 -0.47034988]\n",
    "  [ 0.1410485  -1.3254068 ]]\n",
    "\n",
    " [[-0.9808096   1.3569971 ]\n",
    "  [-1.6258174  -0.23133498]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
