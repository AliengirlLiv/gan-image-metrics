{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3c2d116622b8>:117: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(num_conv >= 2, \"Number of conv layers in the discriminator must be >= 2 b/c I'm laxy.\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, resample=None, normalize=False, activation=None, \n",
    "                 first_activation=True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.in_filters = in_filters\n",
    "        self.out_filters = out_filters\n",
    "        self.resample = resample\n",
    "        self.normalize = normalize\n",
    "        self.conv1 = self.conv_layer(in_filters, out_filters, padding=1)\n",
    "        self.conv2 = self.conv_layer(out_filters, out_filters, padding=1)\n",
    "        if resample:\n",
    "            self.conv3 = self.conv_layer(in_filters, out_filters, kernel_size=1)\n",
    "        if normalize:\n",
    "            self.bn1 = self.bn(in_filters)\n",
    "            self.bn2 = self.bn(out_filters)\n",
    "            \n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "            \n",
    "        self.first_activation = first_activation     \n",
    "    \n",
    "    def forward(self, x): \n",
    "        orig_input = x\n",
    "        if self.normalize:\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "        if self.first_activation:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        if self.resample == 'up':\n",
    "            x = self.upsample(x)\n",
    "       \n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.bn2(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        if self.resample == 'down':\n",
    "            x = self.downsample(x)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.resample == 'down': \n",
    "            shortcut_x = self.downsample(orig_input)\n",
    "            shortcut_x = self.conv3(shortcut_x)\n",
    "        elif self.resample == 'up':\n",
    "            shortcut_x = self.upsample(orig_input)\n",
    "            shortcut_x = self.conv3(shortcut_x)\n",
    "        elif self.resample == None:\n",
    "            shortcut_x = orig_input\n",
    "        return x + shortcut_x\n",
    "    \n",
    "    \n",
    "    def upsample(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\") #TODO: Check this is the same\n",
    "\n",
    "    def downsample(self, x):\n",
    "        batch, height, width, channels = x.shape\n",
    "        return nn.functional.interpolate(x, scale_factor=0.5, \n",
    "                                         mode=\"bilinear\") #TODO: Check this is the same\n",
    "\n",
    "    def conv_layer(self, in_filters, out_filters=32, kernel_size=3, padding=0):\n",
    "        return nn.Conv2d(in_filters, out_filters, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def bn(self, channels):\n",
    "        return nn.BatchNorm2d(channels, eps=1e-5)\n",
    "    \n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, num_filters=128, num_conv=3, first_linear_size=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.first_linear = nn.Linear(input_size, first_linear_size * first_linear_size * num_filters)\n",
    "        self.num_filters = num_filters\n",
    "        self.activation = nn.ReLU()\n",
    "        self.first_linear_size = first_linear_size\n",
    "        self.resblocks = []\n",
    "        for _ in range(num_conv):\n",
    "            self.resblocks.append(\n",
    "                ResBlock(in_filters=self.num_filters, \n",
    "                         out_filters=self.num_filters, \n",
    "                         resample='up', \n",
    "                         normalize=True,\n",
    "                         activation=self.activation))\n",
    "        orig_num_channels = 3\n",
    "        self.last_layer = nn.Conv2d(num_filters, orig_num_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_filters, eps=1e-5)\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        x = self.first_linear(noise)\n",
    "        x = x.view(-1, self.num_filters, self.first_linear_size, self.first_linear_size)\n",
    "        for resblock in self.resblocks:\n",
    "            x = resblock(x)\n",
    "            \n",
    "        x = self.activation(self.bn(x))\n",
    "        result = self.last_layer(x)\n",
    "        return torch.tanh(result)\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_filters=128, num_conv=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        assert(num_conv >= 2, \"Number of conv layers in the discriminator must be >= 2 b/c I'm laxy.\")\n",
    "        \n",
    "        orig_num_filters = 3\n",
    "        self.resblocks = []\n",
    "        self.resblocks.append(ResBlock(in_filters=orig_num_filters, \n",
    "                     out_filters=self.num_filters, \n",
    "                     resample='down', \n",
    "                     normalize=False,\n",
    "                     activation=self.activation,\n",
    "                     first_activation=False))\n",
    "        self.resblocks.append(ResBlock(in_filters=self.num_filters, \n",
    "                     out_filters=self.num_filters, \n",
    "                     resample='down', \n",
    "                     normalize=True,\n",
    "                     activation=self.activation))\n",
    "        for layer in range(num_conv - 2):\n",
    "            self.resblocks.append(ResBlock(in_filters=self.num_filters, \n",
    "                         out_filters=self.num_filters, \n",
    "                         normalize=True,\n",
    "                         activation=self.activation))\n",
    "            \n",
    "        self.last_linear = nn.Linear(num_filters, 1) #TODO: Choose size better\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        for resblock in self.resblocks:\n",
    "            x = resblock(x)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        x = torch.mean(torch.mean(x, dim=3), dim=2)\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.last_linear(x) #\n",
    "        return x\n",
    "        \n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, input_size=32, num_filters=128, num_conv_generator=3, num_conv_discriminator=4,\n",
    "                 c=5, s=0, batch_size=64,\n",
    "                 channels=3, exponent=2):  \n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = Generator(input_size, num_filters=num_filters, num_conv=num_conv_generator)\n",
    "        self.discriminator = Discriminator(num_filters=num_filters, num_conv=num_conv_discriminator)\n",
    "        self.c = c\n",
    "        self.s = s\n",
    "        self.batch_size = batch_size\n",
    "        self.channels = channels\n",
    "        self.exponent = exponent\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward_train_generator(self, noise=None):\n",
    "        generated_image = self.forward_predict_generator(noise)\n",
    "        discriminator_score = self.forward_predict_discriminator(generated_image)\n",
    "        return self.generator_loss(discriminator_score)\n",
    "    \n",
    "    def forward_train_discriminator(self, real_images, noise=None):\n",
    "        generated_images = self.forward_predict_generator(noise)\n",
    "        discriminator_score_generated = self.forward_predict_discriminator(generated_images)\n",
    "        discriminator_score_real = self.forward_predict_discriminator(real_images)\n",
    "        return self.discriminator_loss(discriminator_score_real, discriminator_score_generated, real_images, generated_images)\n",
    "        \n",
    "    def forward_predict_generator(self, noise=None):\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise([self.batch_size, self.input_size])\n",
    "        return self.generator(noise)\n",
    "        \n",
    "    def forward_predict_discriminator(self, image):\n",
    "        return self.discriminator(image)\n",
    "    \n",
    "    def generate_noise(self, tensor_shape):\n",
    "        return torch.randn(tensor_shape)\n",
    "    \n",
    "    def generator_loss(self, d_generated_train):\n",
    "        return torch.mean(d_generated_train) # Technically we should scale by gamma but why bother??\n",
    "    \n",
    "    def discriminator_loss(self, d_score_real, d_score_generated, real_images, generated_images):\n",
    "        gamma = self.calc_gamma(real_images)\n",
    "        lamb = self.calc_lambda(real_images)\n",
    "        \n",
    "        wasserstein_scaled = (torch.mean(d_score_generated) - torch.mean(d_score_real))\n",
    "        wasserstein_loss = wasserstein_scaled / gamma\n",
    "        \n",
    "        epsilon = torch.empty(self.batch_size, 1, 1, 1).uniform_(0, 1)\n",
    "        real_fake_mix = epsilon * generated_images + (1 - epsilon) * real_images \n",
    "        d_score_mix = torch.mean(self.discriminator(real_fake_mix))\n",
    "        \n",
    "        \n",
    "        gradients = []\n",
    "        \n",
    "        def update_grad_variable(grad):\n",
    "            gradients.append(grad)\n",
    "            \n",
    "        real_fake_mix.register_hook(update_grad_variable)    \n",
    "        d_score_mix.backward(retain_graph=True)\n",
    "        gradients = gradients[0]\n",
    "    \n",
    "        dual_sobolev_gradients = self.sobolev_filter(gradients, c=self.c, s=-self.s)\n",
    "        dual_exponent = 1 / (1 - 1/self.exponent) if self.exponent != 1 else np.inf\n",
    "        ddx = self.stable_norm(dual_sobolev_gradients, order=dual_exponent)\n",
    "\n",
    "        d_regularizer = torch.mean((ddx / gamma - 1) ** 2)\n",
    "        d_regularizer_mean = torch.mean(d_score_real ** 2)\n",
    "        \n",
    "        d_loss = (-wasserstein_loss +\n",
    "          lamb * d_regularizer +\n",
    "          1e-5 * d_regularizer_mean)\n",
    "        \n",
    "        return d_loss\n",
    "    \n",
    "    def calc_gamma(self, real_images):      \n",
    "        # DUAL NORM\n",
    "        sobolev_true = self.sobolev_filter(real_images, c=self.c, s=self.s)\n",
    "        dual_exponent = 1 / (1 - 1/self.exponent) if self.exponent != 1 else np.inf\n",
    "        gamma = torch.mean(self.stable_norm(sobolev_true, order=dual_exponent))\n",
    "        return gamma\n",
    "    \n",
    "    def calc_lambda(self, real_images):\n",
    "        sobolev_true = self.sobolev_filter(real_images, c=self.c, s=self.s)\n",
    "        lamb = torch.mean(self.stable_norm(sobolev_true, order=self.exponent))\n",
    "        return lamb\n",
    "    \n",
    "    def stable_norm(self, x, order):\n",
    "        x = x.view(self.batch_size, -1) \n",
    "        alpha, _ = torch.max(torch.abs(x) + 1e-5, dim=1)\n",
    "        alpha = alpha.unsqueeze(1)\n",
    "        result = alpha * torch.norm(x / alpha, p=order, dim=1)\n",
    "        return result\n",
    "    \n",
    "    def sobolev_filter(self, x, c=5, s=1):\n",
    "        \"\"\"Apply sobolev filter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pytorch tensor of shape B C W H\n",
    "        c : float\n",
    "            Scaling of the coordinate systems (1 / pixel size)\n",
    "        s : float\n",
    "            Order of the Sobolev norm\n",
    "        \"\"\"\n",
    "        fft_dim = 2\n",
    "        fft_x = torch.rfft(x, fft_dim)\n",
    "\n",
    "        shape = fft_x.shape\n",
    "        sx = shape[3]\n",
    "        sy = shape[2]\n",
    "\n",
    "        # Construct meshgrid for the scale\n",
    "        x = torch.FloatTensor(range(sx))\n",
    "        x = torch.min(x, sx - x)\n",
    "        x = x / (sx // 2)\n",
    "        y = torch.FloatTensor(range(sy))\n",
    "        y = torch.min(y, sy - y)\n",
    "        y = y / (sy // 2)\n",
    "        X = x.expand(2, sy, sx).permute(1, 2, 0)\n",
    "        Y = y.expand(2, sx, sy).permute(2, 1, 0)\n",
    "        X = X.unsqueeze(0).unsqueeze(0)\n",
    "        Y = Y.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        scale = (1 + c * (X ** 2 + Y ** 2)) ** (s / 2)\n",
    "\n",
    "        # Compute spatial gradient in fourier space\n",
    "        fft_x = scale * fft_x\n",
    "\n",
    "        result_x = torch.irfft(fft_x, fft_dim)\n",
    "        return result_x\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess: 30 compile\n",
    "    \n",
    "    EACH:\n",
    "        - runs\n",
    "        - Gives correct outuput\n",
    "        - Correct formula\n",
    "        - Comments\n",
    "        okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.data.clamp_(-5, 5)\n",
    "\n",
    "def train(model,\n",
    "            dset_loader,\n",
    "            optimizer,\n",
    "            lr_scheduler=None,\n",
    "            num_epochs=20,\n",
    "            use_cuda=False,\n",
    "            num_discriminator=5):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step(epoch)\n",
    "\n",
    "        # Iterate over data.\n",
    "        for batch in dset_loader:\n",
    "            if use_cuda:\n",
    "                batch = batch.cuda()\n",
    "            \n",
    "            for _ in range(num_discriminator):\n",
    "                loss = model.forward_train_discriminator(batch)\n",
    "                loss.backward()\n",
    "                clip_grads(model)\n",
    "                optimizer.step()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            loss = model.forward_train_generator(batch)\n",
    "            loss.backward()\n",
    "            clip_grads(model)\n",
    "            optimizer.step()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi2277/.local/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9e032f61f66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# kangaroo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a86f62ebf40a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dset_loader, optimizer, lr_scheduler, num_epochs, use_cuda, num_discriminator)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mclip_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3c2d116622b8>\u001b[0m in \u001b[0;36mforward_train_generator\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_train_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_predict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mdiscriminator_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_predict_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3c2d116622b8>\u001b[0m in \u001b[0;36mforward_predict_discriminator\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_predict_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3c2d116622b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3c2d116622b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'down'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "input_size = 32\n",
    "batch_size = 64\n",
    "model = GAN(input_size)\n",
    "x_train = [torch.randn(batch_size, 3, 32, 32) for _ in range(3)]\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=500, factor=0.8)\n",
    "num_epochs = 3\n",
    "use_cuda=False\n",
    "\n",
    "import time\n",
    "train(model, x_train, optimizer, lr_scheduler, num_epochs, use_cuda)\n",
    "\n",
    "# kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utilities for computing the sobolev norm.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def sobolev_filter(x, c=5, s=1):\n",
    "    \"\"\"Apply sobolev filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensorflow.Tensor of shape B W H C\n",
    "        txt\n",
    "    c : float\n",
    "        Scaling of the cooridinate systems (1 / pixel size)\n",
    "    s : float\n",
    "        Order of the Sobolev norm\n",
    "    \"\"\"\n",
    "    with tf.name_scope('sobolev'):\n",
    "        # FFT is taken over the innermost axes, so move channel to beginning.\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        fft_x = tf.spectral.fft2d(tf.cast(x, 'complex64'))\n",
    "\n",
    "        shape = tf.shape(fft_x)\n",
    "        sx = shape[3]\n",
    "        sy = shape[2]\n",
    "\n",
    "        # Construct meshgrid for the scale\n",
    "        x = tf.range(sx)\n",
    "        x = tf.minimum(x, sx - x)\n",
    "        x = tf.cast(x, dtype='complex64') / tf.cast(sx // 2, dtype='complex64')\n",
    "        y = tf.range(sy)\n",
    "        y = tf.minimum(y, sy - y)\n",
    "        y = tf.cast(y, dtype='complex64') / tf.cast(sy // 2, dtype='complex64')\n",
    "        X, Y = tf.meshgrid(x, y)\n",
    "        X = X[None, None]\n",
    "        Y = Y[None, None]\n",
    "\n",
    "        scale = (1 + c * (X ** 2 + Y ** 2)) ** (s / 2)\n",
    "\n",
    "        # Compute spatial gradient in fourier space\n",
    "        fft_x = scale * fft_x\n",
    "\n",
    "        result_x = tf.spectral.ifft2d(fft_x)\n",
    "        result_x = tf.real(result_x)\n",
    "        return tf.transpose(result_x, [0, 2, 3, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM ADLER - https://github.com/adler-j/adler/blob/master/adler/tensorflow/summaries.py\n",
    "\n",
    "def scalars_summary(name, x):\n",
    "    with tf.name_scope(name):\n",
    "        x = tf.reshape(x, [-1])\n",
    "        mean, var = tf.nn.moments(x, axes=0)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        tf.summary.scalar('std', tf.sqrt(var))\n",
    "        tf.summary.histogram('histogram', x)\n",
    "        \n",
    "        \n",
    "def image_grid(x, size=8):\n",
    "    t = tf.unstack(x[:size * size], num=size*size, axis=0)\n",
    "    rows = [tf.concat(t[i*size:(i+1)*size], axis=0) for i in range(size)]\n",
    "    image = tf.concat(rows, axis=1)\n",
    "    return image[None]\n",
    "        \n",
    "def image_grid_summary(name, x):\n",
    "    with tf.name_scope(name):\n",
    "        tf.summary.image('grid', image_grid(x))\n",
    "        \n",
    "def default_checkpoint_path(name):\n",
    "    checkpoint_dir = join(get_base_dir(), 'checkpoints')\n",
    "    if not exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    checkpoint_path = join(checkpoint_dir,\n",
    "                           '{}.ckpt'.format(name))\n",
    "\n",
    "    return checkpoint_path\n",
    "\n",
    "def default_tensorboard_dir(name):\n",
    "    tensorboard_dir = join(get_base_dir(), 'tensorboard', name)\n",
    "    if not exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "    return tensorboard_dir\n",
    "\n",
    "def get_base_dir():\n",
    "    \"\"\"Get the data directory.\"\"\"\n",
    "    base_odl_dir = os.environ.get('ADLER_HOME',\n",
    "                                  expanduser(join('~', '.adler')))\n",
    "    data_home = join(base_odl_dir, 'tensorflow')\n",
    "    if not exists(data_home):\n",
    "        os.makedirs(data_home)\n",
    "    return data_home\n",
    "\n",
    "\n",
    "def summary_writers(name, cleanup=False, session=None, write_graph=True):\n",
    "    if session is None:\n",
    "        session = tf.get_default_session()\n",
    "\n",
    "    dname = default_tensorboard_dir(name)\n",
    "\n",
    "    if cleanup and os.path.exists(dname):\n",
    "        shutil.rmtree(dname, ignore_errors=True)\n",
    "\n",
    "    if write_graph:\n",
    "        graph = session.graph\n",
    "    else:\n",
    "        graph = None\n",
    "\n",
    "    test_summary_writer = tf.summary.FileWriter(dname + '/test', graph)\n",
    "    train_summary_writer = tf.summary.FileWriter(dname + '/train')\n",
    "\n",
    "    return test_summary_writer, train_summary_writer\n",
    "\n",
    "class EMAHelper(object):\n",
    "    def __init__(self, decay=0.99, session=None):\n",
    "        if session is None:\n",
    "            self.session = tf.get_default_session()\n",
    "        else:\n",
    "            self.session = session\n",
    "\n",
    "        self.all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "        self.apply = self.ema.apply(self.all_vars)\n",
    "        self.averages = [self.ema.average(var) for var in self.all_vars]\n",
    "\n",
    "    def average_dict(self):\n",
    "        ema_averages_results = self.session.run(self.averages)\n",
    "        return {var: value for var, value in zip(self.all_vars,\n",
    "                                                 ema_averages_results)}\n",
    "\n",
    "    def variables_to_restore(self):\n",
    "        return self.ema.variables_to_restore(tf.moving_average_variables())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code for training Banach Wasserstein GAN on CIFAR 10.\n",
    "\n",
    "With all the dependencies installed, the code should run as-is. \n",
    "Data is downloaded on the fly.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensordata\n",
    "import functools\n",
    "\n",
    "# User selectable parameters\n",
    "EXPONENT = 2\n",
    "SOBOLEV_C = 5.0\n",
    "SOBOLEV_S = 0\n",
    "MAX_ITERS = 100000\n",
    "SUMMARY_FREQ = 10\n",
    "INCEPTION_FREQ = 1000\n",
    "BATCH_SIZE = 64\n",
    "BATCH_SIZE_TEST = 100\n",
    "reset = True\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Training specific parameters\n",
    "size = 32\n",
    "DUAL_EXPONENT = 1 / (1 - 1/EXPONENT) if EXPONENT != 1 else np.inf\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_train_ph, _ = tensordata.get_cifar10_tf(batch_size=BATCH_SIZE)\n",
    "    print(\"SHAPE\", x_train_ph.shape)\n",
    "    x_test_ph, _ = tensordata.get_cifar10_tf(batch_size=BATCH_SIZE_TEST)\n",
    "\n",
    "    is_training = tf.placeholder(bool, name='is_training')\n",
    "    use_agumentation = tf.identity(is_training, name='is_training')\n",
    "\n",
    "\n",
    "with tf.name_scope('pre_process'):\n",
    "    x_train = (x_train_ph - 0.5) * 2.0\n",
    "    x_test = (x_test_ph - 0.5) * 2.0\n",
    "\n",
    "    x_true = tf.cond(is_training,\n",
    "                     lambda: x_train,\n",
    "                     lambda: x_test)\n",
    "\n",
    "def apply_conv(x, filters=32, kernel_size=3, he_init=True):\n",
    "    if he_init:\n",
    "        initializer = tf.contrib.layers.variance_scaling_initializer(uniform=True)\n",
    "    else:\n",
    "        initializer = tf.contrib.layers.xavier_initializer(uniform=True)\n",
    "\n",
    "    return tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size,\n",
    "                            padding='SAME', kernel_initializer=initializer)\n",
    "\n",
    "\n",
    "def activation(x):\n",
    "    with tf.name_scope('activation'):\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def bn(x):\n",
    "    return tf.contrib.layers.batch_norm(x,\n",
    "                                    decay=0.9,\n",
    "                                    center=True,\n",
    "                                    scale=True,\n",
    "                                    epsilon=1e-5,\n",
    "                                    zero_debias_moving_mean=True,\n",
    "                                    is_training=is_training)\n",
    "\n",
    "\n",
    "def stable_norm(x, ord):\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    alpha = tf.reduce_max(tf.abs(x) + 1e-5, axis=1)\n",
    "    result = alpha * tf.norm(x / alpha[:, None], ord=ord, axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def downsample(x):\n",
    "    with tf.name_scope('downsample'):\n",
    "        x = tf.identity(x)\n",
    "        return tf.add_n([x[:,::2,::2,:], x[:,1::2,::2,:],\n",
    "                         x[:,::2,1::2,:], x[:,1::2,1::2,:]]) / 4.\n",
    "\n",
    "def upsample(x):\n",
    "    with tf.name_scope('upsample'):\n",
    "        x = tf.identity(x)\n",
    "        x = tf.concat([x, x, x, x], axis=-1)\n",
    "        return tf.depth_to_space(x, 2)\n",
    "\n",
    "\n",
    "def conv_meanpool(x, **kwargs):\n",
    "    return downsample(apply_conv(x, **kwargs))\n",
    "\n",
    "def meanpool_conv(x, **kwargs):\n",
    "    return apply_conv(downsample(x), **kwargs)\n",
    "\n",
    "def upsample_conv(x, **kwargs):\n",
    "    return apply_conv(upsample(x), **kwargs)\n",
    "\n",
    "def resblock(x, filters, resample=None, normalize=False):\n",
    "    if normalize:\n",
    "        norm_fn = bn\n",
    "    else:\n",
    "        norm_fn = tf.identity\n",
    "\n",
    "    if resample == 'down':\n",
    "        conv_1 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_2 = functools.partial(conv_meanpool, filters=filters)\n",
    "        conv_shortcut = functools.partial(conv_meanpool, filters=filters,\n",
    "                                          kernel_size=1, he_init=False)\n",
    "    elif resample == 'up':\n",
    "        conv_1 = functools.partial(upsample_conv, filters=filters)\n",
    "        conv_2 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_shortcut = functools.partial(upsample_conv, filters=filters,\n",
    "                                          kernel_size=1, he_init=False)\n",
    "    elif resample == None:\n",
    "        conv_1 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_2 = functools.partial(apply_conv, filters=filters)\n",
    "        conv_shortcut = tf.identity\n",
    "\n",
    "    with tf.name_scope('resblock'):\n",
    "        x = tf.identity(x)\n",
    "        update = conv_1(activation(norm_fn(x)))\n",
    "        update = conv_2(activation(norm_fn(update)))\n",
    "\n",
    "        skip = conv_shortcut(x)\n",
    "        return skip + update\n",
    "\n",
    "\n",
    "def resblock_optimized(x, filters):\n",
    "    with tf.name_scope('resblock'):\n",
    "        x = tf.identity(x)\n",
    "        update = apply_conv(x, filters=filters)\n",
    "        update = conv_meanpool(activation(update), filters=filters)\n",
    "\n",
    "        skip = meanpool_conv(x, filters=128, kernel_size=1, he_init=False)\n",
    "        return skip + update\n",
    "\n",
    "\n",
    "def generator(z, reuse):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        with tf.name_scope('pre_process'):\n",
    "            z = tf.layers.dense(z, 4 * 4 * 128)\n",
    "            x = tf.reshape(z, [-1, 4, 4, 128])\n",
    "\n",
    "        with tf.name_scope('x1'):\n",
    "            x = resblock(x, filters=128, resample='up', normalize=True) # 8\n",
    "            x = resblock(x, filters=128, resample='up', normalize=True) # 16\n",
    "            x = resblock(x, filters=128, resample='up', normalize=True) # 32\n",
    "\n",
    "        with tf.name_scope('post_process'):\n",
    "            x = activation(bn(x))\n",
    "            result = apply_conv(x, filters=3, he_init=False)\n",
    "            return tf.tanh(result)\n",
    "\n",
    "\n",
    "def discriminator(x, reuse):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        with tf.name_scope('pre_process'):\n",
    "            x = resblock_optimized(x, filters=128)\n",
    "\n",
    "        with tf.name_scope('x1'):\n",
    "            x = resblock(x, filters=128, resample='down') # 8\n",
    "            x = resblock(x, filters=128) # 16\n",
    "            x = resblock(x, filters=128) # 32\n",
    "\n",
    "        with tf.name_scope('post_process'):\n",
    "            x = activation(x)\n",
    "            x = tf.reduce_mean(x, axis=[1, 2])\n",
    "            flat = tf.contrib.layers.flatten(x)\n",
    "            flat = tf.layers.dense(flat, 1)\n",
    "            return flat\n",
    "\n",
    "\n",
    "with tf.name_scope('gan'):\n",
    "    z = tf.random_normal([tf.shape(x_true)[0], 128], name=\"z\")\n",
    "\n",
    "    x_generated = generator(z, reuse=True) # ORIGINALLY FALSE\n",
    "\n",
    "    d_true = discriminator(x_true, reuse=True) # ORIGINALLY FALSE\n",
    "    d_generated = discriminator(x_generated, reuse=True)\n",
    "\n",
    "    z_gen = tf.random_normal([BATCH_SIZE * 2, 128], name=\"z\")\n",
    "    d_generated_train = discriminator(generator(z_gen, reuse=True), reuse=True)\n",
    "\n",
    "with tf.name_scope('dual_norm'):\n",
    "    sobolev_true = sobolev_filter(x_true, c=SOBOLEV_C, s=SOBOLEV_S)\n",
    "    lamb = tf.reduce_mean(stable_norm(sobolev_true, ord=EXPONENT))\n",
    "    dual_sobolev_true = sobolev_filter(x_true, c=SOBOLEV_C, s=-SOBOLEV_S)\n",
    "    gamma = tf.reduce_mean(stable_norm(sobolev_true, ord=DUAL_EXPONENT))\n",
    "\n",
    "with tf.name_scope('regularizer'):\n",
    "    epsilon = tf.random_uniform([tf.shape(x_true)[0], 1, 1, 1], 0.0, 1.0)\n",
    "    x_hat = epsilon * x_generated + (1 - epsilon) * x_true\n",
    "    d_hat = discriminator(x_hat, reuse=True)\n",
    "\n",
    "    gradients = tf.gradients(d_hat, x_hat)[0]\n",
    "    dual_sobolev_gradients = sobolev_filter(gradients, c=SOBOLEV_C, s=-SOBOLEV_S)\n",
    "    ddx = stable_norm(dual_sobolev_gradients, ord=DUAL_EXPONENT)\n",
    "\n",
    "    d_regularizer = tf.reduce_mean(tf.square(ddx / gamma - 1))\n",
    "    d_regularizer_mean = tf.reduce_mean(tf.square(d_true))\n",
    "\n",
    "with tf.name_scope('loss_gan'):\n",
    "    wasserstein_scaled = (tf.reduce_mean(d_generated) - tf.reduce_mean(d_true))\n",
    "    wasserstein = wasserstein_scaled / gamma\n",
    "\n",
    "    g_loss = tf.reduce_mean(d_generated_train) / gamma\n",
    "    d_loss = (-wasserstein +\n",
    "              lamb * d_regularizer +\n",
    "              1e-5 * d_regularizer_mean)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    ema = EMAHelper(decay=0.99)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    decay = tf.maximum(0., 1.-(tf.cast(global_step, tf.float32)/MAX_ITERS))\n",
    "    learning_rate = 2e-4 * decay\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0., beta2=0.9)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='gan/generator')\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        g_train = optimizer.minimize(g_loss, var_list=g_vars,\n",
    "                                     global_step=global_step)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='gan/discriminator')\n",
    "    d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        d_train = optimizer.minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('wasserstein_scaled', wasserstein_scaled)\n",
    "    tf.summary.scalar('wasserstein', wasserstein)\n",
    "\n",
    "    tf.summary.scalar('g_loss', g_loss)\n",
    "\n",
    "    tf.summary.scalar('d_loss', d_loss)\n",
    "    scalars_summary('d_true', d_true)\n",
    "    scalars_summary('d_generated', d_generated)\n",
    "    tf.summary.scalar('d_regularizer', d_regularizer)\n",
    "    tf.summary.scalar('d_regularizer_mean', d_regularizer_mean)\n",
    "\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    tf.summary.scalar('global_step', global_step)\n",
    "\n",
    "    scalars_summary('x_generated', x_generated)\n",
    "    scalars_summary('x_true', x_true)\n",
    "\n",
    "    scalars_summary('gamma', gamma)\n",
    "    scalars_summary('lamb', lamb)\n",
    "\n",
    "    image_grid_summary('x_true', x_true)\n",
    "    image_grid_summary('x_generated', x_generated)\n",
    "    image_grid_summary('gradients', gradients)\n",
    "    image_grid_summary('dual_sobolev_gradients', dual_sobolev_gradients)\n",
    "\n",
    "    scalars_summary('ddx', ddx)\n",
    "    scalars_summary('gradients', gradients)\n",
    "    scalars_summary('dual_sobolev_gradients', dual_sobolev_gradients)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Advanced metrics\n",
    "    with tf.name_scope('inception'):\n",
    "        # Specific function to compute inception score for very large\n",
    "        # number of samples\n",
    "        def generate_and_classify(z):\n",
    "            INCEPTION_OUTPUT = 'logits:0'\n",
    "            x = generator(z, reuse=True)\n",
    "            x = tf.image.resize_bilinear(x, [299, 299])\n",
    "            return tf.contrib.gan.eval.run_inception(x, output_tensor=INCEPTION_OUTPUT)\n",
    "\n",
    "        # Fixed z for fairness between runs\n",
    "        inception_z = tf.constant(np.random.randn(10000, 128), dtype='float32')\n",
    "        inception_score = tf.contrib.gan.eval.classifier_score(inception_z,\n",
    "                                                               classifier_fn=generate_and_classify,\n",
    "                                                               num_batches=10000 // 100)\n",
    "\n",
    "        inception_summary = tf.summary.merge([\n",
    "                tf.summary.scalar('inception_score', inception_score)])\n",
    "\n",
    "        full_summary = tf.summary.merge([merged_summary, inception_summary])\n",
    "\n",
    "    test_summary_writer, train_summary_writer = summary_writers(name, cleanup=reset, write_graph=False)\n",
    "\n",
    "# Initialize all TF variables\n",
    "sess.run([tf.global_variables_initializer(),\n",
    "          tf.local_variables_initializer()])\n",
    "\n",
    "# Coordinate the loading of image files.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "# Add op to save and restore\n",
    "saver = tf.train.Saver()\n",
    "if not reset:\n",
    "    saver.restore(sess, default_checkpoint_path(name))\n",
    "\n",
    "# Standardized validation z\n",
    "z_validate = np.random.randn(BATCH_SIZE_TEST, 128)\n",
    "\n",
    "# Train the network\n",
    "while True:\n",
    "    i = sess.run(global_step)\n",
    "    if i >= MAX_ITERS:\n",
    "        break\n",
    "\n",
    "    num_d_train = 5\n",
    "    for j in range(num_d_train):\n",
    "        _, d_loss_result = sess.run([d_train, d_loss],\n",
    "                                    feed_dict={is_training: True})\n",
    "\n",
    "    _, g_loss_result, _ = sess.run([g_train, g_loss, ema.apply],\n",
    "             feed_dict={is_training: True})\n",
    "\n",
    "    print('s={}, i={}, j={}, d_loss={}, g_loss={}'.format(SOBOLEV_S, i, j,\n",
    "                                                    d_loss_result,\n",
    "                                                    g_loss_result))\n",
    "\n",
    "    if i % SUMMARY_FREQ == SUMMARY_FREQ - 1:\n",
    "        ema_dict = ema.average_dict()\n",
    "        merged_summary_result_train = sess.run(merged_summary,\n",
    "                                         feed_dict={is_training: False,\n",
    "                                                    **ema_dict})\n",
    "        train_summary_writer.add_summary(merged_summary_result_train, i)\n",
    "    if i % INCEPTION_FREQ == INCEPTION_FREQ - 1:\n",
    "        ema_dict = ema.average_dict()\n",
    "        merged_summary_result_test = sess.run(full_summary,\n",
    "                                         feed_dict={z: z_validate,\n",
    "                                                    is_training: False,\n",
    "                                                    **ema_dict})\n",
    "        test_summary_writer.add_summary(merged_summary_result_test, i)\n",
    "\n",
    "\n",
    "    if i % 1000 == 999:\n",
    "        saver.save(sess, default_checkpoint_path(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
